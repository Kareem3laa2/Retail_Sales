# ğŸ“ˆ Retail Sales Analytics Pipeline: Orchestrated with Airflow & dbt

This project delivers a production-ready **Data Engineering pipeline** that transforms raw retail sales data into analytics-ready insights. It combines the power of **Apache Airflow** for workflow orchestration and **dbt (Data Build Tool)** for modular, SQL-based transformationsâ€”built entirely within a containerized **Docker Compose** environment.

The pipeline implements best practices in **modern data stack design**, including structured data modeling (star schema), automated scheduling, and end-to-end reproducibility. Ideal for both local development and cloud migration readiness.


---

## ğŸš€ Project Overview

This project aims to establish a robust and scalable data pipeline that transforms disparate retail sales data into a clean, structured, and analytics-ready format. By leveraging industry-standard tools, it enables efficient reporting and deeper insights into sales performance.

### ğŸ”‘ Key Features
* **Fully Containerized Environment:** Seamless setup and portability with Docker Compose (Airflow, dbt, Snowflake/PostgreSQL).
* **Automated Workflow Orchestration:** Scheduled execution of data transformations using Apache Airflow.
* **Modular Data Transformation:** Structured dbt project with best practices for SQL-based data modeling.
* **Dimensional Modeling (Star Schema):** Optimized data warehouse design for efficient analytical querying.
* **Data Quality & Testing (via dbt):** Implicitly covered by dbt's capabilities, worth mentioning if tests are implemented.

---

## ğŸ› ï¸ Tech Stack

| Tool                      | Role                               |
| ------------------------- | ---------------------------------- |
| **Apache Airflow**        | Workflow orchestration             |
| **Apache Spark**          | Data processing and Transformation |
| **Hadoop HDFS**           | Acts as a datalake for (Bronze & Silver) | 
| **dbt (Data Build Tool)** | SQL-based data transformations     |
| **Docker Compose**        | Containerized development          |
| **Snowflake/PostgreSQL**  | Data warehouse / target backend    |
| **Microsoft PowerBI**     | Data Visualization                 |
| **GitHub**                | Version control & CI/CD (optional) |

---

## ğŸ—ºï¸ Architecture Diagram

A visual representation of the data flow and component interactions within the pipeline.

![Architecture Diagram](images/Retail_Sales_P1_Design_Arch.png)
*Figure 1: High-level overview of the Retail Sales Data Pipeline Architecture.*

---
## ğŸ“Š Retail Sales Analytics Dashboard
![Architecture Diagram](images/Retail_Sales_P1_Dashboard.png)
*Figure 2: Gold PowerBI Dashboard Connected to snowflake.*

---

## ğŸ“ Project Structure

```plaintext
RETAIL_SALES/
â”‚
â”œâ”€â”€ docker-compose.yml              # Defines multi-container Docker application
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ dbt_dag.py                  # Airflow DAG for orchestrating dbt transformations
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ retail_sales_project/
â”‚       â”œâ”€â”€ dbt_project.yml         # dbt project configuration
â”‚       â”œâ”€â”€ models/                 # dbt SQL models (staging, intermediate, marts)
â”‚       â”‚   â”œâ”€â”€ staging/            # Raw data transformation into staging tables
â”‚       â”‚   â”œâ”€â”€ marts/              # Final aggregated and dimensional models (e.g., Star Schema)
â”‚       â”‚   â””â”€â”€ ...                 # Other model layers as needed
â”‚       â””â”€â”€ ...                     # Other dbt artifacts (macros, tests, seeds, etc.)
â”œâ”€â”€ data/                           # Placeholder for raw input data (e.g., CSVs)
â”œâ”€â”€ logs/                           # Airflow, Spark, or dbt log files
â”œâ”€â”€ jars/                           # Custom JARs for Spark (if applicable)
â”œâ”€â”€ notebooks/                      # Jupyter notebooks for data exploration or analysis
â”œâ”€â”€ spark/                          # Spark application code or configurations (if Spark is used)
â””â”€â”€ README.md
```

---

## âš™ï¸ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/retail-sales-pipeline.git
cd retail-sales-pipeline
```

### 2ï¸âƒ£ Start Docker Compose

```bash
docker-compose up --build
```

### 3ï¸âƒ£ Access Airflow

- Web UI: [http://localhost:8080](http://localhost:8080)
- Default Login: `airflow / airflow`

### 4ï¸âƒ£ Trigger the DAG

- Open Airflow UI
- Enable and trigger `retail_pipeline`

---

## ğŸ“Š Data Modeling Layers

- **Staging Layer**: Cleans and standardizes raw data stored as view in snowflake
- **Mart Layer**: Star Schema with Fact and Dimension tables

### Dimensions and facts created

- `dim_customer`, `dim_product`, `dim_country`, `dim_date`, `fact_sales`

---
