{"timestamp":"2025-07-18T08:00:49.898866","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-18T08:00:49.899478","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/upload_to_hdfs_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-18T08:00:49.917657","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:49.918486","level":"info","event":"Running command: ['/usr/bin/bash', '-c', 'docker exec retail-spark-master                        /opt/bitnami/spark/bin/spark-submit                        --master spark://spark-master:7077 /opt/spark-apps/scripts/snowflake_staging.py']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:49.927525","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:51.973480","level":"info","event":"25/07/18 08:00:51 INFO SparkContext: Running Spark version 3.3.3","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:51.999434","level":"info","event":"25/07/18 08:00:51 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:51.999658","level":"info","event":"25/07/18 08:00:51 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:51.999762","level":"info","event":"25/07/18 08:00:51 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.000256","level":"info","event":"25/07/18 08:00:51 INFO SparkContext: Submitted application: Retail_Snowflake_Staging","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.019781","level":"info","event":"25/07/18 08:00:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.026927","level":"info","event":"25/07/18 08:00:52 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.027368","level":"info","event":"25/07/18 08:00:52 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.065879","level":"info","event":"25/07/18 08:00:52 INFO SecurityManager: Changing view acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.066066","level":"info","event":"25/07/18 08:00:52 INFO SecurityManager: Changing modify acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.066552","level":"info","event":"25/07/18 08:00:52 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.067006","level":"info","event":"25/07/18 08:00:52 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.067399","level":"info","event":"25/07/18 08:00:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.116263","level":"info","event":"25/07/18 08:00:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.333436","level":"info","event":"25/07/18 08:00:52 INFO Utils: Successfully started service 'sparkDriver' on port 42907.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.357783","level":"info","event":"25/07/18 08:00:52 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.388932","level":"info","event":"25/07/18 08:00:52 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.401287","level":"info","event":"25/07/18 08:00:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.401458","level":"info","event":"25/07/18 08:00:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.405506","level":"info","event":"25/07/18 08:00:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.420141","level":"info","event":"25/07/18 08:00:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-aa6c1a34-4fb7-413a-86f3-0db4e36fd838","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.434241","level":"info","event":"25/07/18 08:00:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.449623","level":"info","event":"25/07/18 08:00:52 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.627150","level":"info","event":"25/07/18 08:00:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.761601","level":"info","event":"25/07/18 08:00:52 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://retail-spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.803377","level":"info","event":"25/07/18 08:00:52 INFO TransportClientFactory: Successfully created connection to retail-spark-master/172.24.0.4:7077 after 26 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.904769","level":"info","event":"25/07/18 08:00:52 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250718080052-0003","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.907005","level":"info","event":"25/07/18 08:00:52 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250718080052-0003/0 on worker-20250718075613-172.24.0.6-34031 (172.24.0.6:34031) with 12 core(s)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.908801","level":"info","event":"25/07/18 08:00:52 INFO StandaloneSchedulerBackend: Granted executor ID app-20250718080052-0003/0 on hostPort 172.24.0.6:34031 with 12 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.910049","level":"info","event":"25/07/18 08:00:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35697.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.910628","level":"info","event":"25/07/18 08:00:52 INFO NettyBlockTransferService: Server created on d22432c79974:35697","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.913161","level":"info","event":"25/07/18 08:00:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.922570","level":"info","event":"25/07/18 08:00:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, d22432c79974, 35697, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.926015","level":"info","event":"25/07/18 08:00:52 INFO BlockManagerMasterEndpoint: Registering block manager d22432c79974:35697 with 434.4 MiB RAM, BlockManagerId(driver, d22432c79974, 35697, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.928378","level":"info","event":"25/07/18 08:00:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, d22432c79974, 35697, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.930007","level":"info","event":"25/07/18 08:00:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, d22432c79974, 35697, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:52.947607","level":"info","event":"25/07/18 08:00:52 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250718080052-0003/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:53.097968","level":"info","event":"25/07/18 08:00:53 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:53.343004","level":"info","event":"25/07/18 08:00:53 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:53.352425","level":"info","event":"25/07/18 08:00:53 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:54.856364","level":"info","event":"25/07/18 08:00:54 INFO InMemoryFileIndex: It took 96 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.224428","level":"info","event":"25/07/18 08:00:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.24.0.6:42558) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.232215","level":"info","event":"25/07/18 08:00:55 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.248960","level":"info","event":"25/07/18 08:00:55 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.251489","level":"info","event":"25/07/18 08:00:55 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.253759","level":"info","event":"25/07/18 08:00:55 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.255892","level":"info","event":"25/07/18 08:00:55 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.260875","level":"info","event":"25/07/18 08:00:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.318371","level":"info","event":"25/07/18 08:00:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.24.0.6:38999 with 434.4 MiB RAM, BlockManagerId(0, 172.24.0.6, 38999, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.332364","level":"info","event":"25/07/18 08:00:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.385517","level":"info","event":"25/07/18 08:00:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.8 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.394713","level":"info","event":"25/07/18 08:00:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on d22432c79974:35697 (size: 36.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.397345","level":"info","event":"25/07/18 08:00:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1509","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.418559","level":"info","event":"25/07/18 08:00:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.418971","level":"info","event":"25/07/18 08:00:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.478515","level":"info","event":"25/07/18 08:00:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.24.0.6, executor 0, partition 0, PROCESS_LOCAL, 4667 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:55.765292","level":"info","event":"25/07/18 08:00:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.24.0.6:38999 (size: 36.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:56.692387","level":"info","event":"25/07/18 08:00:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1233 ms on 172.24.0.6 (executor 0) (1/1)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:56.694741","level":"info","event":"25/07/18 08:00:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:56.699292","level":"info","event":"25/07/18 08:00:56 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.418 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:56.702249","level":"info","event":"25/07/18 08:00:56 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:56.702451","level":"info","event":"25/07/18 08:00:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:56.704163","level":"info","event":"25/07/18 08:00:56 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.472641 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:57.132025","level":"info","event":"25/07/18 08:00:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on d22432c79974:35697 in memory (size: 36.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:57.137951","level":"info","event":"25/07/18 08:00:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.24.0.6:38999 in memory (size: 36.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:58.339034","level":"info","event":"25/07/18 08:00:58 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:58.340442","level":"info","event":"25/07/18 08:00:58 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:58.342824","level":"info","event":"25/07/18 08:00:58 INFO FileSourceStrategy: Output Data Schema: struct<invoice_no: string, stock_code: string, description: string, quantity: int, invoice_date: string ... 8 more fields>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:58.542542","level":"info","event":"25/07/18 08:00:58 INFO CodeGenerator: Code generated in 142.548661 ms","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:58.566756","level":"info","event":"25/07/18 08:00:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 201.8 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:58.579076","level":"info","event":"25/07/18 08:00:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:58.580262","level":"info","event":"25/07/18 08:00:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on d22432c79974:35697 (size: 34.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:58.581567","level":"info","event":"25/07/18 08:00:58 INFO SparkContext: Created broadcast 1 from rdd at SnowflakeWriter.scala:87","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:58.591933","level":"info","event":"25/07/18 08:00:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4489130 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:00:58.657262","level":"info","event":"25/07/18 08:00:58 WARN ServerConnection$: JDBC 3.13.31 is being used. But the certified JDBC version 3.13.30 is recommended.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.035926","level":"info","event":"25/07/18 08:01:00 INFO ServerConnection$: Create ServerConnection with new JDBC connection: 182007829176386","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.049572","level":"info","event":"25/07/18 08:01:00 INFO SparkConnectorContext$: Spark Connector system config: {","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.049768","level":"info","event":"  \"spark_connector_version\" : \"2.12.0\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.049848","level":"info","event":"  \"spark_version\" : \"3.3.3\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.049890","level":"info","event":"  \"application_name\" : \"Retail_Snowflake_Staging\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.049934","level":"info","event":"  \"scala_version\" : \"2.12.15\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.049968","level":"info","event":"  \"java_version\" : \"17.0.9\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050008","level":"info","event":"  \"jdbc_version\" : \"3.13.31\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050053","level":"info","event":"  \"certified_jdbc_version\" : \"3.13.30\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050091","level":"info","event":"  \"os_name\" : \"Linux\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050128","level":"info","event":"  \"max_memory_in_mb\" : 1024,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050159","level":"info","event":"  \"total_memory_in_mb\" : 229,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050210","level":"info","event":"  \"free_memory_in_mb\" : 118,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050262","level":"info","event":"  \"cpu_cores\" : 12,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050308","level":"info","event":"  \"spark_application_id\" : \"app-20250718080052-0003\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050340","level":"info","event":"  \"spark_language\" : \"Scala\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050399","level":"info","event":"  \"is_pyspark\" : false,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050443","level":"info","event":"  \"spark_config\" : {","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050481","level":"info","event":"    \"spark.sql.warehouse.dir\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050525","level":"info","event":"    \"spark.driver.host\" : \"d22432c79974\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050563","level":"info","event":"    \"spark.app.name\" : \"Retail_Snowflake_Staging\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050604","level":"info","event":"    \"spark.app.id\" : \"app-20250718080052-0003\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050638","level":"info","event":"    \"spark.app.startTime\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050679","level":"info","event":"    \"spark.executor.id\" : \"driver\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050720","level":"info","event":"    \"spark.driver.port\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050788","level":"info","event":"    \"spark.driver.extraJavaOptions\" : \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-exports java.base/sun.nio.ch=ALL-UNNAMED\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050843","level":"info","event":"    \"spark.rdd.compress\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050879","level":"info","event":"    \"spark.executor.extraJavaOptions\" : \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.050910","level":"info","event":"    \"spark.serializer.objectStreamReset\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.051022","level":"info","event":"    \"spark.submit.pyFiles\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.051060","level":"info","event":"    \"spark.submit.deployMode\" : \"client\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.051128","level":"info","event":"    \"spark.master\" : \"spark://retail-spark-master:7077\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.051191","level":"info","event":"    \"spark.app.submitTime\" : \"N/A\"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.051266","level":"info","event":"  },","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.051305","level":"info","event":"  \"libraries\" : [ \"py4j\", \"py4j.commands\", \"py4j.reflection\" ],","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.051358","level":"info","event":"  \"dependencies\" : [ ],","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.051395","level":"info","event":"  \"cluster_node_count\" : 2,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.051424","level":"info","event":"  \"spark_default_parallelism\" : 12,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.051476","level":"info","event":"  \"deploy_mode\" : \"client\"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.051513","level":"info","event":"}","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.058745","level":"info","event":"25/07/18 08:01:00 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: alter session set timezone = 'Etc/UTC' , timestamp_ntz_output_format = 'YYYY-MM-DD HH24:MI:SS.FF3', timestamp_ltz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3', timestamp_tz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3' ;","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:00.679084","level":"info","event":"25/07/18 08:01:00 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: create  temporary stage if not exists identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.489992","level":"info","event":"25/07/18 08:01:01 INFO CloudStorageOperations$: Spark Connector Master: Begin to process and upload data for 12 partitions: directory=bciAtRveep CSV true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.490188","level":"info","event":"25/07/18 08:01:01 INFO CloudStorageOperations$: Spark Connector Master: Begin to retrieve pre-signed URL or down-scoped token for 12 files by calling PUT command.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.801914","level":"info","event":"25/07/18 08:01:01 INFO CloudStorageOperations$: Spark Connector Master: Upload file to GCP with down-scoped token instead of pre-signed URL.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.802984","level":"info","event":"25/07/18 08:01:01 INFO CloudStorageOperations$: Spark Connector Master: Time to retrieve down-scoped token for 1/12 files is 312 ms.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.878732","level":"info","event":"25/07/18 08:01:01 INFO SparkContext: Starting job: collect at CloudStorageOperations.scala:1862","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.880227","level":"info","event":"25/07/18 08:01:01 INFO DAGScheduler: Got job 1 (collect at CloudStorageOperations.scala:1862) with 12 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.880409","level":"info","event":"25/07/18 08:01:01 INFO DAGScheduler: Final stage: ResultStage 1 (collect at CloudStorageOperations.scala:1862)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.880478","level":"info","event":"25/07/18 08:01:01 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.880706","level":"info","event":"25/07/18 08:01:01 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.881743","level":"info","event":"25/07/18 08:01:01 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at mapPartitionsWithIndex at CloudStorageOperations.scala:1839), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.891483","level":"info","event":"25/07/18 08:01:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 39.6 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.896438","level":"info","event":"25/07/18 08:01:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.0 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.897535","level":"info","event":"25/07/18 08:01:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on d22432c79974:35697 (size: 19.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.898578","level":"info","event":"25/07/18 08:01:01 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.899526","level":"info","event":"25/07/18 08:01:01 INFO DAGScheduler: Submitting 12 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at mapPartitionsWithIndex at CloudStorageOperations.scala:1839) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.899695","level":"info","event":"25/07/18 08:01:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 12 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.906949","level":"info","event":"25/07/18 08:01:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.24.0.6, executor 0, partition 0, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.907142","level":"info","event":"25/07/18 08:01:01 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.24.0.6, executor 0, partition 1, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.907230","level":"info","event":"25/07/18 08:01:01 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (172.24.0.6, executor 0, partition 2, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.907283","level":"info","event":"25/07/18 08:01:01 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (172.24.0.6, executor 0, partition 3, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.908093","level":"info","event":"25/07/18 08:01:01 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (172.24.0.6, executor 0, partition 4, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.908986","level":"info","event":"25/07/18 08:01:01 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (172.24.0.6, executor 0, partition 5, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.909116","level":"info","event":"25/07/18 08:01:01 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (172.24.0.6, executor 0, partition 6, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.909355","level":"info","event":"25/07/18 08:01:01 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (172.24.0.6, executor 0, partition 7, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.909576","level":"info","event":"25/07/18 08:01:01 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (172.24.0.6, executor 0, partition 8, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.909829","level":"info","event":"25/07/18 08:01:01 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (172.24.0.6, executor 0, partition 9, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.910596","level":"info","event":"25/07/18 08:01:01 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (172.24.0.6, executor 0, partition 10, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.912330","level":"info","event":"25/07/18 08:01:01 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (172.24.0.6, executor 0, partition 11, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:01.957094","level":"info","event":"25/07/18 08:01:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.24.0.6:38999 (size: 19.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.578007","level":"info","event":"25/07/18 08:01:03 ERROR Inbox: Ignoring error","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.578207","level":"info","event":"java.io.NotSerializableException: org.apache.spark.storage.StorageStatus","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.578281","level":"info","event":"Serialization stack:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.578325","level":"info","event":"\t- object not serializable (class: org.apache.spark.storage.StorageStatus, value: org.apache.spark.storage.StorageStatus@5df6790)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.578638","level":"info","event":"\t- element of array (index: 0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.578706","level":"info","event":"\t- array (class [Lorg.apache.spark.storage.StorageStatus;, size 2)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.578751","level":"info","event":"\tat org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.578795","level":"info","event":"\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:49)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.578836","level":"info","event":"\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.578876","level":"info","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:286)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.578912","level":"info","event":"\tat org.apache.spark.rpc.netty.RemoteNettyRpcCallContext.send(NettyRpcCallContext.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.579051","level":"info","event":"\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.579100","level":"info","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:162)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.579143","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.579183","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.579214","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.579244","level":"info","event":"\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.579276","level":"info","event":"\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.579309","level":"info","event":"\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.579348","level":"info","event":"\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.579400","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.579445","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.579482","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:03.635642","level":"info","event":"25/07/18 08:01:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.24.0.6:38999 (size: 34.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:10.566619","level":"info","event":"25/07/18 08:01:10 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 8654 ms on 172.24.0.6 (executor 0) (1/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:13.363135","level":"info","event":"25/07/18 08:01:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 11457 ms on 172.24.0.6 (executor 0) (2/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:18.528163","level":"info","event":"25/07/18 08:01:18 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 16618 ms on 172.24.0.6 (executor 0) (3/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:22.214019","level":"info","event":"25/07/18 08:01:22 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 20303 ms on 172.24.0.6 (executor 0) (4/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:22.413352","level":"info","event":"25/07/18 08:01:22 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 20503 ms on 172.24.0.6 (executor 0) (5/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:22.624388","level":"info","event":"25/07/18 08:01:22 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 20713 ms on 172.24.0.6 (executor 0) (6/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:22.909513","level":"info","event":"25/07/18 08:01:22 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 21002 ms on 172.24.0.6 (executor 0) (7/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:23.075178","level":"info","event":"25/07/18 08:01:23 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 21165 ms on 172.24.0.6 (executor 0) (8/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:23.126412","level":"info","event":"25/07/18 08:01:23 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 21217 ms on 172.24.0.6 (executor 0) (9/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:23.334118","level":"info","event":"25/07/18 08:01:23 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 21423 ms on 172.24.0.6 (executor 0) (10/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:23.522228","level":"info","event":"25/07/18 08:01:23 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 21615 ms on 172.24.0.6 (executor 0) (11/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:24.674330","level":"info","event":"25/07/18 08:01:24 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 22767 ms on 172.24.0.6 (executor 0) (12/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:24.674530","level":"info","event":"25/07/18 08:01:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:24.675468","level":"info","event":"25/07/18 08:01:24 INFO DAGScheduler: ResultStage 1 (collect at CloudStorageOperations.scala:1862) finished in 22.787 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:24.675574","level":"info","event":"25/07/18 08:01:24 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:24.675635","level":"info","event":"25/07/18 08:01:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:24.675887","level":"info","event":"25/07/18 08:01:24 INFO DAGScheduler: Job 1 finished: collect at CloudStorageOperations.scala:1862, took 22.960752 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:24.678731","level":"info","event":"25/07/18 08:01:24 INFO CloudStorageOperations$: Spark Connector Master: Finish uploading data for 12 partitions in 23.19 seconds.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:24.679551","level":"info","event":"25/07/18 08:01:24 INFO StageWriter$: writeToTableWithStagingTable: check table existence with \"RETAIL_SALES\".\"STAGING\".SILVER_DATA for SILVER_DATA","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:24.680364","level":"info","event":"25/07/18 08:01:24 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: desc table identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:24.865789","level":"info","event":"25/07/18 08:01:24 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: create   table if not exists identifier(?) (\"INVOICE_NO\" STRING ,\"STOCK_CODE\" STRING ,\"DESCRIPTION\" STRING ,\"QUANTITY\" INTEGER ,\"INVOICE_DATE\" STRING ,\"UNIT_PRICE\" DOUBLE ,\"CUSTOMER_ID\" STRING ,\"COUNTRY\" STRING ,\"REVENUE\" DOUBLE ,\"IS_RETURN\" INTEGER )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.143444","level":"info","event":"25/07/18 08:01:25 INFO StageWriter$: Begin to write at 2025-07-18T08:01:25.135130266 (Coordinated Universal Time)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.144751","level":"info","event":"25/07/18 08:01:25 INFO StageWriter$: Total file count is 12, non-empty files count is 12, total file size is 36.82 MB, total row count is 397.29 KB.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.147590","level":"info","event":"25/07/18 08:01:25 INFO StageWriter$: Now executing below command to write into table:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.147881","level":"info","event":"copy into SILVER_DATA_staging_722921025 FROM @spark_connector_load_stage_qKCL0CdMjU/bciAtRveep/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148001","level":"info","event":"FILE_FORMAT = (","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148083","level":"info","event":"    TYPE=CSV","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148157","level":"info","event":"    FIELD_DELIMITER='|'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148239","level":"info","event":"    NULL_IF=()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148317","level":"info","event":"    FIELD_OPTIONALLY_ENCLOSED_BY='\"'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148407","level":"info","event":"    TIMESTAMP_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148483","level":"info","event":"    DATE_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148538","level":"info","event":"    BINARY_FORMAT=BASE64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148593","level":"info","event":"  )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148641","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148697","level":"info","event":"25/07/18 08:01:25 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: copy into SILVER_DATA_staging_722921025 FROM @spark_connector_load_stage_qKCL0CdMjU/bciAtRveep/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148745","level":"info","event":"FILE_FORMAT = (","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148790","level":"info","event":"    TYPE=CSV","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148834","level":"info","event":"    FIELD_DELIMITER='|'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148879","level":"info","event":"    NULL_IF=()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.148960","level":"info","event":"    FIELD_OPTIONALLY_ENCLOSED_BY='\"'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.149036","level":"info","event":"    TIMESTAMP_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.149090","level":"info","event":"    DATE_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.149139","level":"info","event":"    BINARY_FORMAT=BASE64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.149188","level":"info","event":"  )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.149235","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.314844","level":"info","event":"25/07/18 08:01:25 INFO SparkConnectorContext$: Spark connector register listener for: app-20250718080052-0003","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.316039","level":"info","event":"25/07/18 08:01:25 INFO SparkConnectorContext$: Add running query for app-20250718080052-0003 session: 182007829176386 queryId: 01bdc441-0000-a176-0000-a58900013256","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.319744","level":"info","event":"25/07/18 08:01:25 INFO StageWriter$: The query ID for async writing into table command is: 01bdc441-0000-a176-0000-a58900013256; The query ID URL is:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:25.320158","level":"info","event":"https://TRLIYPI-XK63730.snowflakecomputing.com/console#/monitoring/queries/detail?queryId=01bdc441-0000-a176-0000-a58900013256","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:28.822085","level":"info","event":"25/07/18 08:01:28 INFO SparkConnectorContext$: Remove running query for app-20250718080052-0003 session: 182007829176386 queryId: 01bdc441-0000-a176-0000-a58900013256","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:28.822403","level":"info","event":"25/07/18 08:01:28 INFO StageWriter$: First COPY command is done in 3.69 seconds at 2025-07-18T08:01:28.821196086, queryID is 01bdc441-0000-a176-0000-a58900013256","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:28.827245","level":"info","event":"25/07/18 08:01:28 INFO StageWriter$: Succeed to write in 3.69 seconds at 2025-07-18T08:01:28.825739832","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:28.828569","level":"info","event":"25/07/18 08:01:28 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: alter table identifier(?) swap with identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:29.015522","level":"info","event":"25/07/18 08:01:29 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: drop table identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:29.271316","level":"info","event":"25/07/18 08:01:29 INFO StageWriter$: Spark Connector Master: Total job time is 27.78 seconds including read & upload time: 23.19 seconds and COPY time: 4.59 seconds.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:29.276131","level":"info","event":"25/07/18 08:01:29 INFO ServerConnection$: Create ServerConnection with cached JDBC connection: 182007829176386","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:29.277160","level":"info","event":"25/07/18 08:01:29 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: select * from SILVER_DATA where 1 = 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.273309","level":"info","event":"25/07/18 08:01:30 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.274718","level":"info","event":"25/07/18 08:01:30 WARN SparkConnectorContext$: Finish cancelling all queries for app-20250718080052-0003","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.275058","level":"info","event":"25/07/18 08:01:30 INFO ServerConnection$: Close all 1 cached connection.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.282624","level":"info","event":"25/07/18 08:01:30 INFO SparkUI: Stopped Spark web UI at http://d22432c79974:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.285054","level":"info","event":"25/07/18 08:01:30 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.285284","level":"info","event":"25/07/18 08:01:30 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.568385","level":"info","event":"25/07/18 08:01:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.589418","level":"info","event":"25/07/18 08:01:30 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.589877","level":"info","event":"25/07/18 08:01:30 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.593404","level":"info","event":"25/07/18 08:01:30 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.596861","level":"info","event":"25/07/18 08:01:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.608399","level":"info","event":"25/07/18 08:01:30 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.608768","level":"info","event":"25/07/18 08:01:30 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.609788","level":"info","event":"25/07/18 08:01:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-e4ebeae3-10ac-4831-a195-52307f209ec9","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.614072","level":"info","event":"25/07/18 08:01:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-272e5724-125e-4cfa-bbf8-e689baa03a5a","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.618595","level":"info","event":"25/07/18 08:01:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-272e5724-125e-4cfa-bbf8-e689baa03a5a/pyspark-d7ba5200-38f4-4d02-8c76-863703831d11","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.667364","level":"info","event":"Command exited with return code 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T08:01:30.668141","level":"info","event":"Pushing xcom","ti":"RuntimeTaskInstance(id=UUID('01981c8c-9c1f-7bf2-a94f-7043adb9fdbb'), task_id='silver_layer_snowflake', dag_id='retail_pipeline', run_id='manual__2025-07-18T08:00:33.800206+00:00', try_number=1, map_index=-1, hostname='cee2c2032bfa', context_carrier=None, task=<Task(BashOperator): silver_layer_snowflake>, bundle_instance=LocalDagBundle(name=dags-folder), max_tries=0, start_date=datetime.datetime(2025, 7, 18, 8, 0, 49, 850934, tzinfo=TzInfo(UTC)), end_date=None, is_mapped=False)","logger":"task"}
