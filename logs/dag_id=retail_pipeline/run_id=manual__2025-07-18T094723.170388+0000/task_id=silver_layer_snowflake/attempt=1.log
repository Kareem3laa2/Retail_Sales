{"timestamp":"2025-07-18T09:47:39.983346","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-18T09:47:39.983927","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/upload_to_hdfs_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-18T09:47:40.003208","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:40.003966","level":"info","event":"Running command: ['/usr/bin/bash', '-c', 'docker exec retail-spark-master                        /opt/bitnami/spark/bin/spark-submit                        --master spark://spark-master:7077 /opt/spark-apps/scripts/snowflake_staging.py']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:40.014604","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.235603","level":"info","event":"25/07/18 09:47:42 INFO SparkContext: Running Spark version 3.4.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.260346","level":"info","event":"25/07/18 09:47:42 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.260663","level":"info","event":"25/07/18 09:47:42 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.261380","level":"info","event":"25/07/18 09:47:42 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.261790","level":"info","event":"25/07/18 09:47:42 INFO SparkContext: Submitted application: Retail_Snowflake_Staging","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.283900","level":"info","event":"25/07/18 09:47:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.290597","level":"info","event":"25/07/18 09:47:42 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.291094","level":"info","event":"25/07/18 09:47:42 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.331204","level":"info","event":"25/07/18 09:47:42 INFO SecurityManager: Changing view acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.331370","level":"info","event":"25/07/18 09:47:42 INFO SecurityManager: Changing modify acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.331440","level":"info","event":"25/07/18 09:47:42 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.331750","level":"info","event":"25/07/18 09:47:42 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.332103","level":"info","event":"25/07/18 09:47:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.386275","level":"info","event":"25/07/18 09:47:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.587683","level":"info","event":"25/07/18 09:47:42 INFO Utils: Successfully started service 'sparkDriver' on port 40945.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.609213","level":"info","event":"25/07/18 09:47:42 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.640549","level":"info","event":"25/07/18 09:47:42 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.655097","level":"info","event":"25/07/18 09:47:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.655783","level":"info","event":"25/07/18 09:47:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.660073","level":"info","event":"25/07/18 09:47:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.677558","level":"info","event":"25/07/18 09:47:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3dcdb1f6-6a35-49d2-8438-4762ee9a83a5","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.690827","level":"info","event":"25/07/18 09:47:42 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.704460","level":"info","event":"25/07/18 09:47:42 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.804216","level":"info","event":"25/07/18 09:47:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.857946","level":"info","event":"25/07/18 09:47:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.959617","level":"info","event":"25/07/18 09:47:42 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://retail-spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:42.994583","level":"info","event":"25/07/18 09:47:42 INFO TransportClientFactory: Successfully created connection to retail-spark-master/172.24.0.4:7077 after 19 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.063879","level":"info","event":"25/07/18 09:47:43 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250718094743-0019","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.065359","level":"info","event":"25/07/18 09:47:43 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250718094743-0019/0 on worker-20250718090909-172.24.0.6-42597 (172.24.0.6:42597) with 12 core(s)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.066750","level":"info","event":"25/07/18 09:47:43 INFO StandaloneSchedulerBackend: Granted executor ID app-20250718094743-0019/0 on hostPort 172.24.0.6:42597 with 12 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.070047","level":"info","event":"25/07/18 09:47:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33267.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.070186","level":"info","event":"25/07/18 09:47:43 INFO NettyBlockTransferService: Server created on 8da053d44e9f:33267","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.072897","level":"info","event":"25/07/18 09:47:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.077677","level":"info","event":"25/07/18 09:47:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8da053d44e9f, 33267, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.080745","level":"info","event":"25/07/18 09:47:43 INFO BlockManagerMasterEndpoint: Registering block manager 8da053d44e9f:33267 with 434.4 MiB RAM, BlockManagerId(driver, 8da053d44e9f, 33267, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.082764","level":"info","event":"25/07/18 09:47:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8da053d44e9f, 33267, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.084176","level":"info","event":"25/07/18 09:47:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8da053d44e9f, 33267, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.096052","level":"info","event":"25/07/18 09:47:43 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250718094743-0019/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.261341","level":"info","event":"25/07/18 09:47:43 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.482076","level":"info","event":"25/07/18 09:47:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:43.490688","level":"info","event":"25/07/18 09:47:43 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:44.827331","level":"info","event":"25/07/18 09:47:44 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:45.030328","level":"info","event":"25/07/18 09:47:45 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.24.0.6:45324) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:45.081965","level":"info","event":"25/07/18 09:47:45 INFO BlockManagerMasterEndpoint: Registering block manager 172.24.0.6:36221 with 434.4 MiB RAM, BlockManagerId(0, 172.24.0.6, 36221, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:46.337040","level":"info","event":"25/07/18 09:47:46 WARN SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.4.0 with a connector designed to support Spark 3.3. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:46.407492","level":"info","event":"25/07/18 09:47:46 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:46.416944","level":"info","event":"25/07/18 09:47:46 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:46.595592","level":"info","event":"25/07/18 09:47:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.3 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:46.640218","level":"info","event":"25/07/18 09:47:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:46.642673","level":"info","event":"25/07/18 09:47:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8da053d44e9f:33267 (size: 34.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:46.645683","level":"info","event":"25/07/18 09:47:46 INFO SparkContext: Created broadcast 0 from rdd at SnowflakeWriter.scala:87","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:46.660789","level":"info","event":"25/07/18 09:47:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7576840 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:46.761402","level":"info","event":"25/07/18 09:47:46 WARN ServerConnection$: JDBC 3.13.31 is being used. But the certified JDBC version 3.13.30 is recommended.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.516384","level":"info","event":"25/07/18 09:47:48 INFO ServerConnection$: Create ServerConnection with new JDBC connection: 182007829172302","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535315","level":"info","event":"25/07/18 09:47:48 INFO SparkConnectorContext$: Spark Connector system config: {","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535478","level":"info","event":"  \"spark_connector_version\" : \"2.12.0\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535567","level":"info","event":"  \"spark_version\" : \"3.4.0\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535620","level":"info","event":"  \"application_name\" : \"Retail_Snowflake_Staging\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535666","level":"info","event":"  \"scala_version\" : \"2.12.17\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535705","level":"info","event":"  \"java_version\" : \"17.0.7\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535751","level":"info","event":"  \"jdbc_version\" : \"3.13.31\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535782","level":"info","event":"  \"certified_jdbc_version\" : \"3.13.30\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535836","level":"info","event":"  \"os_name\" : \"Linux\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535873","level":"info","event":"  \"max_memory_in_mb\" : 1024,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535914","level":"info","event":"  \"total_memory_in_mb\" : 170,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535946","level":"info","event":"  \"free_memory_in_mb\" : 74,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.535976","level":"info","event":"  \"cpu_cores\" : 12,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536018","level":"info","event":"  \"spark_application_id\" : \"app-20250718094743-0019\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536103","level":"info","event":"  \"spark_language\" : \"Scala\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536163","level":"info","event":"  \"is_pyspark\" : false,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536219","level":"info","event":"  \"spark_config\" : {","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536257","level":"info","event":"    \"spark.app.id\" : \"app-20250718094743-0019\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536298","level":"info","event":"    \"spark.sql.warehouse.dir\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536329","level":"info","event":"    \"spark.app.name\" : \"Retail_Snowflake_Staging\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536370","level":"info","event":"    \"spark.executor.id\" : \"driver\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536402","level":"info","event":"    \"spark.app.submitTime\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536443","level":"info","event":"    \"spark.driver.host\" : \"8da053d44e9f\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536480","level":"info","event":"    \"spark.app.startTime\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536513","level":"info","event":"    \"spark.driver.extraJavaOptions\" : \"-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false --add-exports java.base/sun.nio.ch=ALL-UNNAMED\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536583","level":"info","event":"    \"spark.rdd.compress\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536688","level":"info","event":"    \"spark.serializer.objectStreamReset\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536744","level":"info","event":"    \"spark.submit.pyFiles\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536778","level":"info","event":"    \"spark.driver.port\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536859","level":"info","event":"    \"spark.submit.deployMode\" : \"client\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536928","level":"info","event":"    \"spark.master\" : \"spark://retail-spark-master:7077\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.536980","level":"info","event":"    \"spark.executor.extraJavaOptions\" : \"-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.537042","level":"info","event":"  },","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.537086","level":"info","event":"  \"libraries\" : [ \"py4j\", \"py4j.commands\", \"py4j.reflection\" ],","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.537160","level":"info","event":"  \"dependencies\" : [ ],","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.537225","level":"info","event":"  \"cluster_node_count\" : 2,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.537269","level":"info","event":"  \"spark_default_parallelism\" : 12,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.537311","level":"info","event":"  \"deploy_mode\" : \"client\"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.537350","level":"info","event":"}","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:48.544803","level":"info","event":"25/07/18 09:47:48 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: alter session set timezone = 'Etc/UTC' , timestamp_ntz_output_format = 'YYYY-MM-DD HH24:MI:SS.FF3', timestamp_ltz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3', timestamp_tz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3' ;","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:49.203089","level":"info","event":"25/07/18 09:47:49 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: create  temporary stage if not exists identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.309720","level":"info","event":"25/07/18 09:47:50 INFO CloudStorageOperations$: Spark Connector Master: Begin to process and upload data for 11 partitions: directory=6UKIgVMVeh CSV true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.309854","level":"info","event":"25/07/18 09:47:50 INFO CloudStorageOperations$: Spark Connector Master: Begin to retrieve pre-signed URL or down-scoped token for 11 files by calling PUT command.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.623322","level":"info","event":"25/07/18 09:47:50 INFO CloudStorageOperations$: Spark Connector Master: Upload file to GCP with down-scoped token instead of pre-signed URL.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.624999","level":"info","event":"25/07/18 09:47:50 INFO CloudStorageOperations$: Spark Connector Master: Time to retrieve down-scoped token for 1/11 files is 314 ms.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.744088","level":"info","event":"25/07/18 09:47:50 INFO SparkContext: Starting job: collect at CloudStorageOperations.scala:1862","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.757804","level":"info","event":"25/07/18 09:47:50 INFO DAGScheduler: Got job 0 (collect at CloudStorageOperations.scala:1862) with 11 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.757980","level":"info","event":"25/07/18 09:47:50 INFO DAGScheduler: Final stage: ResultStage 0 (collect at CloudStorageOperations.scala:1862)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.759194","level":"info","event":"25/07/18 09:47:50 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.760539","level":"info","event":"25/07/18 09:47:50 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.764329","level":"info","event":"25/07/18 09:47:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at mapPartitionsWithIndex at CloudStorageOperations.scala:1839), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.786099","level":"info","event":"25/07/18 09:47:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 34.6 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.805901","level":"info","event":"25/07/18 09:47:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.808382","level":"info","event":"25/07/18 09:47:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8da053d44e9f:33267 (size: 18.7 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.809344","level":"info","event":"25/07/18 09:47:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.829737","level":"info","event":"25/07/18 09:47:50 INFO DAGScheduler: Submitting 11 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at mapPartitionsWithIndex at CloudStorageOperations.scala:1839) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.831029","level":"info","event":"25/07/18 09:47:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 11 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.875468","level":"info","event":"25/07/18 09:47:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.24.0.6, executor 0, partition 0, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.878920","level":"info","event":"25/07/18 09:47:50 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.24.0.6, executor 0, partition 1, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.880267","level":"info","event":"25/07/18 09:47:50 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.24.0.6, executor 0, partition 2, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.880500","level":"info","event":"25/07/18 09:47:50 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.24.0.6, executor 0, partition 3, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.880598","level":"info","event":"25/07/18 09:47:50 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.24.0.6, executor 0, partition 4, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.882164","level":"info","event":"25/07/18 09:47:50 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.24.0.6, executor 0, partition 5, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.882412","level":"info","event":"25/07/18 09:47:50 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.24.0.6, executor 0, partition 6, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.883784","level":"info","event":"25/07/18 09:47:50 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.24.0.6, executor 0, partition 7, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.883910","level":"info","event":"25/07/18 09:47:50 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.24.0.6, executor 0, partition 8, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.883964","level":"info","event":"25/07/18 09:47:50 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.24.0.6, executor 0, partition 9, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:50.884006","level":"info","event":"25/07/18 09:47:50 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.24.0.6, executor 0, partition 10, ANY, 8152 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:51.153147","level":"info","event":"25/07/18 09:47:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.24.0.6:36221 (size: 18.7 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111278","level":"info","event":"25/07/18 09:47:53 ERROR Inbox: Ignoring error","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111503","level":"info","event":"java.io.NotSerializableException: org.apache.spark.storage.StorageStatus","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111587","level":"info","event":"Serialization stack:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111628","level":"info","event":"\t- object not serializable (class: org.apache.spark.storage.StorageStatus, value: org.apache.spark.storage.StorageStatus@2f656a0a)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111658","level":"info","event":"\t- element of array (index: 0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111685","level":"info","event":"\t- array (class [Lorg.apache.spark.storage.StorageStatus;, size 2)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111714","level":"info","event":"\tat org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111740","level":"info","event":"\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:49)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111763","level":"info","event":"\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111786","level":"info","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:286)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111810","level":"info","event":"\tat org.apache.spark.rpc.netty.RemoteNettyRpcCallContext.send(NettyRpcCallContext.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111839","level":"info","event":"\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111865","level":"info","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:162)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111896","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111937","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.111985","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.112038","level":"info","event":"\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.112090","level":"info","event":"\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.112139","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.112189","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.112234","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:833)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:47:53.203026","level":"info","event":"25/07/18 09:47:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.24.0.6:36221 (size: 34.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:10.790312","level":"info","event":"25/07/18 09:48:10 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 19906 ms on 172.24.0.6 (executor 0) (1/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:11.758515","level":"info","event":"25/07/18 09:48:11 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 20876 ms on 172.24.0.6 (executor 0) (2/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:12.866676","level":"info","event":"25/07/18 09:48:12 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 21985 ms on 172.24.0.6 (executor 0) (3/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:13.022736","level":"info","event":"25/07/18 09:48:13 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 22142 ms on 172.24.0.6 (executor 0) (4/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:14.072911","level":"info","event":"25/07/18 09:48:14 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 23189 ms on 172.24.0.6 (executor 0) (5/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:14.092331","level":"info","event":"25/07/18 09:48:14 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 23210 ms on 172.24.0.6 (executor 0) (6/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:14.290940","level":"info","event":"25/07/18 09:48:14 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 23410 ms on 172.24.0.6 (executor 0) (7/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:14.421477","level":"info","event":"25/07/18 09:48:14 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 23540 ms on 172.24.0.6 (executor 0) (8/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:14.578491","level":"info","event":"25/07/18 09:48:14 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 23700 ms on 172.24.0.6 (executor 0) (9/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:14.700134","level":"info","event":"25/07/18 09:48:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 23841 ms on 172.24.0.6 (executor 0) (10/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:15.831240","level":"info","event":"25/07/18 09:48:15 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 24950 ms on 172.24.0.6 (executor 0) (11/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:15.832022","level":"info","event":"25/07/18 09:48:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:15.832960","level":"info","event":"25/07/18 09:48:15 INFO DAGScheduler: ResultStage 0 (collect at CloudStorageOperations.scala:1862) finished in 25.050 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:15.836128","level":"info","event":"25/07/18 09:48:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:15.836358","level":"info","event":"25/07/18 09:48:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:15.838461","level":"info","event":"25/07/18 09:48:15 INFO DAGScheduler: Job 0 finished: collect at CloudStorageOperations.scala:1862, took 25.093947 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:15.842354","level":"info","event":"25/07/18 09:48:15 INFO CloudStorageOperations$: Spark Connector Master: Finish uploading data for 11 partitions in 25.53 seconds.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:15.842654","level":"info","event":"25/07/18 09:48:15 INFO StageWriter$: writeToTableWithStagingTable: check table existence with \"RETAIL_SALES\".\"STAGING\".SILVER_DATA for SILVER_DATA","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:15.843746","level":"info","event":"25/07/18 09:48:15 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: desc table identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.037867","level":"info","event":"25/07/18 09:48:16 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: create   table if not exists identifier(?) (\"INVOICE_NO\" STRING ,\"STOCK_CODE\" STRING ,\"DESCRIPTION\" STRING ,\"QUANTITY\" INTEGER ,\"INVOICE_DATE\" STRING ,\"UNIT_PRICE\" DOUBLE ,\"CUSTOMER_ID\" STRING ,\"COUNTRY\" STRING ,\"REVENUE\" DOUBLE ,\"IS_RETURN\" INTEGER )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.389746","level":"info","event":"25/07/18 09:48:16 INFO StageWriter$: Begin to write at 2025-07-18T09:48:16.376135134 (Coordinated Universal Time)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.390326","level":"info","event":"25/07/18 09:48:16 INFO StageWriter$: Total file count is 11, non-empty files count is 11, total file size is 43.36 MB, total row count is 397.28 KB.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.394657","level":"info","event":"25/07/18 09:48:16 INFO StageWriter$: Now executing below command to write into table:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.394824","level":"info","event":"copy into SILVER_DATA_staging_1402003988 FROM @spark_connector_load_stage_DWPSe2fa9T/6UKIgVMVeh/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.394965","level":"info","event":"FILE_FORMAT = (","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395077","level":"info","event":"    TYPE=CSV","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395158","level":"info","event":"    FIELD_DELIMITER='|'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395218","level":"info","event":"    NULL_IF=()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395275","level":"info","event":"    FIELD_OPTIONALLY_ENCLOSED_BY='\"'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395329","level":"info","event":"    TIMESTAMP_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395443","level":"info","event":"    DATE_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395542","level":"info","event":"    BINARY_FORMAT=BASE64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395610","level":"info","event":"  )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395662","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395725","level":"info","event":"25/07/18 09:48:16 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: copy into SILVER_DATA_staging_1402003988 FROM @spark_connector_load_stage_DWPSe2fa9T/6UKIgVMVeh/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395802","level":"info","event":"FILE_FORMAT = (","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395871","level":"info","event":"    TYPE=CSV","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.395963","level":"info","event":"    FIELD_DELIMITER='|'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.396033","level":"info","event":"    NULL_IF=()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.396103","level":"info","event":"    FIELD_OPTIONALLY_ENCLOSED_BY='\"'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.396166","level":"info","event":"    TIMESTAMP_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.396210","level":"info","event":"    DATE_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.396258","level":"info","event":"    BINARY_FORMAT=BASE64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.396291","level":"info","event":"  )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.396350","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.566990","level":"info","event":"25/07/18 09:48:16 INFO SparkConnectorContext$: Spark connector register listener for: app-20250718094743-0019","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.572365","level":"info","event":"25/07/18 09:48:16 INFO SparkConnectorContext$: Add running query for app-20250718094743-0019 session: 182007829172302 queryId: 01bdc4ac-0000-a13d-0000-a589000143f2","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.575431","level":"info","event":"25/07/18 09:48:16 INFO StageWriter$: The query ID for async writing into table command is: 01bdc4ac-0000-a13d-0000-a589000143f2; The query ID URL is:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:16.575879","level":"info","event":"https://TRLIYPI-XK63730.snowflakecomputing.com/console#/monitoring/queries/detail?queryId=01bdc4ac-0000-a13d-0000-a589000143f2","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:21.912855","level":"info","event":"25/07/18 09:48:21 INFO SparkConnectorContext$: Remove running query for app-20250718094743-0019 session: 182007829172302 queryId: 01bdc4ac-0000-a13d-0000-a589000143f2","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:21.914068","level":"info","event":"25/07/18 09:48:21 INFO StageWriter$: First COPY command is done in 5.54 seconds at 2025-07-18T09:48:21.912179726, queryID is 01bdc4ac-0000-a13d-0000-a589000143f2","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:21.918752","level":"info","event":"25/07/18 09:48:21 INFO StageWriter$: Succeed to write in 5.54 seconds at 2025-07-18T09:48:21.917277507","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:21.920232","level":"info","event":"25/07/18 09:48:21 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: alter table identifier(?) rename to identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:22.119040","level":"info","event":"25/07/18 09:48:22 INFO StageWriter$: Spark Connector Master: Total job time is 31.80 seconds including read & upload time: 25.53 seconds and COPY time: 6.27 seconds.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:22.122803","level":"info","event":"25/07/18 09:48:22 WARN SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.4.0 with a connector designed to support Spark 3.3. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:22.135391","level":"info","event":"25/07/18 09:48:22 INFO ServerConnection$: Create ServerConnection with cached JDBC connection: 182007829172302","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:22.136899","level":"info","event":"25/07/18 09:48:22 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: select * from SILVER_DATA where 1 = 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.131308","level":"info","event":"25/07/18 09:48:23 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.131946","level":"info","event":"25/07/18 09:48:23 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.136847","level":"info","event":"25/07/18 09:48:23 WARN SparkConnectorContext$: Finish cancelling all queries for app-20250718094743-0019","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.137379","level":"info","event":"25/07/18 09:48:23 INFO ServerConnection$: Close all 1 cached connection.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.160913","level":"info","event":"25/07/18 09:48:23 INFO SparkUI: Stopped Spark web UI at http://8da053d44e9f:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.172202","level":"info","event":"25/07/18 09:48:23 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.172592","level":"info","event":"25/07/18 09:48:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.475954","level":"info","event":"25/07/18 09:48:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.501064","level":"info","event":"25/07/18 09:48:23 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.501325","level":"info","event":"25/07/18 09:48:23 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.506305","level":"info","event":"25/07/18 09:48:23 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.509171","level":"info","event":"25/07/18 09:48:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.518879","level":"info","event":"25/07/18 09:48:23 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.519063","level":"info","event":"25/07/18 09:48:23 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.519571","level":"info","event":"25/07/18 09:48:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-97809a38-eb43-4305-b537-955705c18009/pyspark-0e5b713f-2f81-4582-90ef-0501da4b7219","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.523325","level":"info","event":"25/07/18 09:48:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-97809a38-eb43-4305-b537-955705c18009","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.526710","level":"info","event":"25/07/18 09:48:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-db965db4-d997-49ee-ad44-702b3109bd27","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.574467","level":"info","event":"Command exited with return code 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:48:23.575441","level":"info","event":"Pushing xcom","ti":"RuntimeTaskInstance(id=UUID('01981cee-68b2-739d-9c37-0457968c4c87'), task_id='silver_layer_snowflake', dag_id='retail_pipeline', run_id='manual__2025-07-18T09:47:23.170388+00:00', try_number=1, map_index=-1, hostname='47a9167a254f', context_carrier=None, task=<Task(BashOperator): silver_layer_snowflake>, bundle_instance=LocalDagBundle(name=dags-folder), max_tries=0, start_date=datetime.datetime(2025, 7, 18, 9, 47, 39, 907510, tzinfo=TzInfo(UTC)), end_date=None, is_mapped=False)","logger":"task"}
