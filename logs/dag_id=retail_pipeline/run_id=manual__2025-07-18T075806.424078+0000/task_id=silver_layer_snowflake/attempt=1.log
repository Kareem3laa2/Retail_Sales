{"timestamp":"2025-07-18T07:58:23.722819","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-18T07:58:23.723352","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/upload_to_hdfs_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-18T07:58:23.739058","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:23.739664","level":"info","event":"Running command: ['/usr/bin/bash', '-c', 'docker exec retail-spark-master                        /opt/bitnami/spark/bin/spark-submit                        --master spark://spark-master:7077 /opt/spark-apps/scripts/snowflake_staging.py']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:23.748687","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.672296","level":"info","event":"25/07/18 07:58:25 INFO SparkContext: Running Spark version 3.3.3","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.704377","level":"info","event":"25/07/18 07:58:25 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.704703","level":"info","event":"25/07/18 07:58:25 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.705050","level":"info","event":"25/07/18 07:58:25 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.705479","level":"info","event":"25/07/18 07:58:25 INFO SparkContext: Submitted application: Retail_Snowflake_Staging","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.732056","level":"info","event":"25/07/18 07:58:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.744361","level":"info","event":"25/07/18 07:58:25 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.745058","level":"info","event":"25/07/18 07:58:25 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.793671","level":"info","event":"25/07/18 07:58:25 INFO SecurityManager: Changing view acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.794506","level":"info","event":"25/07/18 07:58:25 INFO SecurityManager: Changing modify acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.796962","level":"info","event":"25/07/18 07:58:25 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.798668","level":"info","event":"25/07/18 07:58:25 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.799748","level":"info","event":"25/07/18 07:58:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:25.853600","level":"info","event":"25/07/18 07:58:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.081763","level":"info","event":"25/07/18 07:58:26 INFO Utils: Successfully started service 'sparkDriver' on port 45247.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.104209","level":"info","event":"25/07/18 07:58:26 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.134930","level":"info","event":"25/07/18 07:58:26 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.150607","level":"info","event":"25/07/18 07:58:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.150990","level":"info","event":"25/07/18 07:58:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.155814","level":"info","event":"25/07/18 07:58:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.171050","level":"info","event":"25/07/18 07:58:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-62496d9f-7817-4794-a145-0855b9c45d22","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.184953","level":"info","event":"25/07/18 07:58:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.197402","level":"info","event":"25/07/18 07:58:26 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.339573","level":"info","event":"25/07/18 07:58:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.444340","level":"info","event":"25/07/18 07:58:26 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://retail-spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.479802","level":"info","event":"25/07/18 07:58:26 INFO TransportClientFactory: Successfully created connection to retail-spark-master/172.24.0.4:7077 after 19 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.563693","level":"info","event":"25/07/18 07:58:26 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250718075826-0001","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.566164","level":"info","event":"25/07/18 07:58:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250718075826-0001/0 on worker-20250718075613-172.24.0.6-34031 (172.24.0.6:34031) with 12 core(s)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.569674","level":"info","event":"25/07/18 07:58:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20250718075826-0001/0 on hostPort 172.24.0.6:34031 with 12 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.570858","level":"info","event":"25/07/18 07:58:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37325.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.570989","level":"info","event":"25/07/18 07:58:26 INFO NettyBlockTransferService: Server created on d22432c79974:37325","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.573648","level":"info","event":"25/07/18 07:58:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.581025","level":"info","event":"25/07/18 07:58:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, d22432c79974, 37325, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.584586","level":"info","event":"25/07/18 07:58:26 INFO BlockManagerMasterEndpoint: Registering block manager d22432c79974:37325 with 434.4 MiB RAM, BlockManagerId(driver, d22432c79974, 37325, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.587168","level":"info","event":"25/07/18 07:58:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, d22432c79974, 37325, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.589316","level":"info","event":"25/07/18 07:58:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, d22432c79974, 37325, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.607516","level":"info","event":"25/07/18 07:58:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250718075826-0001/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.758873","level":"info","event":"25/07/18 07:58:26 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.976081","level":"info","event":"25/07/18 07:58:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:26.984630","level":"info","event":"25/07/18 07:58:26 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.289195","level":"info","event":"25/07/18 07:58:28 INFO InMemoryFileIndex: It took 79 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.622841","level":"info","event":"25/07/18 07:58:28 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.636161","level":"info","event":"25/07/18 07:58:28 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.637528","level":"info","event":"25/07/18 07:58:28 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.640595","level":"info","event":"25/07/18 07:58:28 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.641041","level":"info","event":"25/07/18 07:58:28 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.644691","level":"info","event":"25/07/18 07:58:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.675248","level":"info","event":"25/07/18 07:58:28 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.24.0.6:54076) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.689812","level":"info","event":"25/07/18 07:58:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.7 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.722711","level":"info","event":"25/07/18 07:58:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.8 KiB, free 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.725831","level":"info","event":"25/07/18 07:58:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on d22432c79974:37325 (size: 36.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.729486","level":"info","event":"25/07/18 07:58:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1509","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.740470","level":"info","event":"25/07/18 07:58:28 INFO BlockManagerMasterEndpoint: Registering block manager 172.24.0.6:42687 with 434.4 MiB RAM, BlockManagerId(0, 172.24.0.6, 42687, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.744981","level":"info","event":"25/07/18 07:58:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.745416","level":"info","event":"25/07/18 07:58:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.788408","level":"info","event":"25/07/18 07:58:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.24.0.6, executor 0, partition 0, PROCESS_LOCAL, 4667 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:28.995603","level":"info","event":"25/07/18 07:58:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.24.0.6:42687 (size: 36.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:29.883666","level":"info","event":"25/07/18 07:58:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1104 ms on 172.24.0.6 (executor 0) (1/1)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:29.884998","level":"info","event":"25/07/18 07:58:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:29.889398","level":"info","event":"25/07/18 07:58:29 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.232 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:29.891441","level":"info","event":"25/07/18 07:58:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:29.891757","level":"info","event":"25/07/18 07:58:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:29.893964","level":"info","event":"25/07/18 07:58:29 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.271116 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:30.327926","level":"info","event":"25/07/18 07:58:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on d22432c79974:37325 in memory (size: 36.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:30.335842","level":"info","event":"25/07/18 07:58:30 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.24.0.6:42687 in memory (size: 36.8 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:31.494771","level":"info","event":"25/07/18 07:58:31 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:31.495729","level":"info","event":"25/07/18 07:58:31 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:31.497851","level":"info","event":"25/07/18 07:58:31 INFO FileSourceStrategy: Output Data Schema: struct<invoice_no: string, stock_code: string, description: string, quantity: int, invoice_date: string ... 8 more fields>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:31.687909","level":"info","event":"25/07/18 07:58:31 INFO CodeGenerator: Code generated in 121.903073 ms","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:31.712314","level":"info","event":"25/07/18 07:58:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 201.8 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:31.724912","level":"info","event":"25/07/18 07:58:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:31.725564","level":"info","event":"25/07/18 07:58:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on d22432c79974:37325 (size: 34.9 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:31.726626","level":"info","event":"25/07/18 07:58:31 INFO SparkContext: Created broadcast 1 from rdd at SnowflakeWriter.scala:87","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:31.737606","level":"info","event":"25/07/18 07:58:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4489130 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:31.812628","level":"info","event":"25/07/18 07:58:31 WARN ServerConnection$: JDBC 3.13.31 is being used. But the certified JDBC version 3.13.30 is recommended.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.540870","level":"info","event":"25/07/18 07:58:35 INFO ServerConnection$: Create ServerConnection with new JDBC connection: 182007829176382","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553430","level":"info","event":"25/07/18 07:58:35 INFO SparkConnectorContext$: Spark Connector system config: {","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553535","level":"info","event":"  \"spark_connector_version\" : \"2.12.0\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553574","level":"info","event":"  \"spark_version\" : \"3.3.3\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553607","level":"info","event":"  \"application_name\" : \"Retail_Snowflake_Staging\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553637","level":"info","event":"  \"scala_version\" : \"2.12.15\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553664","level":"info","event":"  \"java_version\" : \"17.0.9\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553687","level":"info","event":"  \"jdbc_version\" : \"3.13.31\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553711","level":"info","event":"  \"certified_jdbc_version\" : \"3.13.30\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553735","level":"info","event":"  \"os_name\" : \"Linux\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553759","level":"info","event":"  \"max_memory_in_mb\" : 1024,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553782","level":"info","event":"  \"total_memory_in_mb\" : 214,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553804","level":"info","event":"  \"free_memory_in_mb\" : 98,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553827","level":"info","event":"  \"cpu_cores\" : 12,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553853","level":"info","event":"  \"spark_application_id\" : \"app-20250718075826-0001\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553878","level":"info","event":"  \"spark_language\" : \"Scala\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553908","level":"info","event":"  \"is_pyspark\" : false,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553947","level":"info","event":"  \"spark_config\" : {","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.553986","level":"info","event":"    \"spark.app.startTime\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554028","level":"info","event":"    \"spark.app.id\" : \"app-20250718075826-0001\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554069","level":"info","event":"    \"spark.sql.warehouse.dir\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554105","level":"info","event":"    \"spark.driver.host\" : \"d22432c79974\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554144","level":"info","event":"    \"spark.app.name\" : \"Retail_Snowflake_Staging\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554178","level":"info","event":"    \"spark.executor.id\" : \"driver\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554209","level":"info","event":"    \"spark.driver.extraJavaOptions\" : \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-exports java.base/sun.nio.ch=ALL-UNNAMED\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554245","level":"info","event":"    \"spark.rdd.compress\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554298","level":"info","event":"    \"spark.executor.extraJavaOptions\" : \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554344","level":"info","event":"    \"spark.app.submitTime\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554379","level":"info","event":"    \"spark.serializer.objectStreamReset\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554418","level":"info","event":"    \"spark.submit.pyFiles\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554459","level":"info","event":"    \"spark.submit.deployMode\" : \"client\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554500","level":"info","event":"    \"spark.master\" : \"spark://retail-spark-master:7077\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554538","level":"info","event":"    \"spark.driver.port\" : \"N/A\"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554602","level":"info","event":"  },","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554662","level":"info","event":"  \"libraries\" : [ \"py4j\", \"py4j.commands\", \"py4j.reflection\" ],","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554753","level":"info","event":"  \"dependencies\" : [ ],","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554793","level":"info","event":"  \"cluster_node_count\" : 2,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554829","level":"info","event":"  \"spark_default_parallelism\" : 12,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554925","level":"info","event":"  \"deploy_mode\" : \"client\"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.554976","level":"info","event":"}","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:35.563071","level":"info","event":"25/07/18 07:58:35 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: alter session set timezone = 'Etc/UTC' , timestamp_ntz_output_format = 'YYYY-MM-DD HH24:MI:SS.FF3', timestamp_ltz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3', timestamp_tz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3' ;","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:36.180375","level":"info","event":"25/07/18 07:58:36 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: create  temporary stage if not exists identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.183864","level":"info","event":"25/07/18 07:58:37 INFO CloudStorageOperations$: Spark Connector Master: Begin to process and upload data for 12 partitions: directory=iNloAmAaQm CSV true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.184052","level":"info","event":"25/07/18 07:58:37 INFO CloudStorageOperations$: Spark Connector Master: Begin to retrieve pre-signed URL or down-scoped token for 12 files by calling PUT command.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.701493","level":"info","event":"25/07/18 07:58:37 INFO CloudStorageOperations$: Spark Connector Master: Upload file to GCP with down-scoped token instead of pre-signed URL.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.703401","level":"info","event":"25/07/18 07:58:37 INFO CloudStorageOperations$: Spark Connector Master: Time to retrieve down-scoped token for 1/12 files is 517 ms.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.766841","level":"info","event":"25/07/18 07:58:37 INFO SparkContext: Starting job: collect at CloudStorageOperations.scala:1862","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.767958","level":"info","event":"25/07/18 07:58:37 INFO DAGScheduler: Got job 1 (collect at CloudStorageOperations.scala:1862) with 12 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.768135","level":"info","event":"25/07/18 07:58:37 INFO DAGScheduler: Final stage: ResultStage 1 (collect at CloudStorageOperations.scala:1862)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.768286","level":"info","event":"25/07/18 07:58:37 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.768458","level":"info","event":"25/07/18 07:58:37 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.769303","level":"info","event":"25/07/18 07:58:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at mapPartitionsWithIndex at CloudStorageOperations.scala:1839), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.777512","level":"info","event":"25/07/18 07:58:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 39.6 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.781519","level":"info","event":"25/07/18 07:58:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.0 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.782536","level":"info","event":"25/07/18 07:58:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on d22432c79974:37325 (size: 19.0 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.783494","level":"info","event":"25/07/18 07:58:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1509","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.784137","level":"info","event":"25/07/18 07:58:37 INFO DAGScheduler: Submitting 12 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at mapPartitionsWithIndex at CloudStorageOperations.scala:1839) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.784263","level":"info","event":"25/07/18 07:58:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 12 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.789483","level":"info","event":"25/07/18 07:58:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.24.0.6, executor 0, partition 0, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.789805","level":"info","event":"25/07/18 07:58:37 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.24.0.6, executor 0, partition 1, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.789971","level":"info","event":"25/07/18 07:58:37 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (172.24.0.6, executor 0, partition 2, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.791143","level":"info","event":"25/07/18 07:58:37 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (172.24.0.6, executor 0, partition 3, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.792191","level":"info","event":"25/07/18 07:58:37 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (172.24.0.6, executor 0, partition 4, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.792504","level":"info","event":"25/07/18 07:58:37 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (172.24.0.6, executor 0, partition 5, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.792630","level":"info","event":"25/07/18 07:58:37 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (172.24.0.6, executor 0, partition 6, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.792678","level":"info","event":"25/07/18 07:58:37 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (172.24.0.6, executor 0, partition 7, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.792713","level":"info","event":"25/07/18 07:58:37 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (172.24.0.6, executor 0, partition 8, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.793085","level":"info","event":"25/07/18 07:58:37 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (172.24.0.6, executor 0, partition 9, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.793392","level":"info","event":"25/07/18 07:58:37 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (172.24.0.6, executor 0, partition 10, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.794062","level":"info","event":"25/07/18 07:58:37 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (172.24.0.6, executor 0, partition 11, ANY, 4990 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:37.839684","level":"info","event":"25/07/18 07:58:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.24.0.6:42687 (size: 19.0 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.343705","level":"info","event":"25/07/18 07:58:39 ERROR Inbox: Ignoring error","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.343860","level":"info","event":"java.io.NotSerializableException: org.apache.spark.storage.StorageStatus","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.343921","level":"info","event":"Serialization stack:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.343962","level":"info","event":"\t- object not serializable (class: org.apache.spark.storage.StorageStatus, value: org.apache.spark.storage.StorageStatus@6062655d)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.343996","level":"info","event":"\t- element of array (index: 0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344076","level":"info","event":"\t- array (class [Lorg.apache.spark.storage.StorageStatus;, size 2)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344153","level":"info","event":"\tat org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344212","level":"info","event":"\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:49)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344255","level":"info","event":"\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344296","level":"info","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:286)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344340","level":"info","event":"\tat org.apache.spark.rpc.netty.RemoteNettyRpcCallContext.send(NettyRpcCallContext.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344371","level":"info","event":"\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344395","level":"info","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:162)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344447","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344481","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344508","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344532","level":"info","event":"\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344558","level":"info","event":"\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344602","level":"info","event":"\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344630","level":"info","event":"\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344655","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344680","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.344768","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:39.414699","level":"info","event":"25/07/18 07:58:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.24.0.6:42687 (size: 34.9 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:41.418112","level":"info","event":"25/07/18 07:58:41 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 3624 ms on 172.24.0.6 (executor 0) (1/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:53.725839","level":"info","event":"25/07/18 07:58:53 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 15932 ms on 172.24.0.6 (executor 0) (2/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:54.903653","level":"info","event":"25/07/18 07:58:54 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 17107 ms on 172.24.0.6 (executor 0) (3/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:56.859639","level":"info","event":"25/07/18 07:58:56 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 19068 ms on 172.24.0.6 (executor 0) (4/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:56.902765","level":"info","event":"25/07/18 07:58:56 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 19108 ms on 172.24.0.6 (executor 0) (5/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:57.377748","level":"info","event":"25/07/18 07:58:57 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 19587 ms on 172.24.0.6 (executor 0) (6/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:58.747661","level":"info","event":"25/07/18 07:58:58 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 20955 ms on 172.24.0.6 (executor 0) (7/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:58.901038","level":"info","event":"25/07/18 07:58:58 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 21105 ms on 172.24.0.6 (executor 0) (8/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:58.967205","level":"info","event":"25/07/18 07:58:58 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 21174 ms on 172.24.0.6 (executor 0) (9/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:59.067322","level":"info","event":"25/07/18 07:58:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 21276 ms on 172.24.0.6 (executor 0) (10/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:58:59.108669","level":"info","event":"25/07/18 07:58:59 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 21317 ms on 172.24.0.6 (executor 0) (11/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.132447","level":"info","event":"25/07/18 07:59:00 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 22336 ms on 172.24.0.6 (executor 0) (12/12)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.133074","level":"info","event":"25/07/18 07:59:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.138756","level":"info","event":"25/07/18 07:59:00 INFO DAGScheduler: ResultStage 1 (collect at CloudStorageOperations.scala:1862) finished in 22.356 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.139272","level":"info","event":"25/07/18 07:59:00 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.139752","level":"info","event":"25/07/18 07:59:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.141316","level":"info","event":"25/07/18 07:59:00 INFO DAGScheduler: Job 1 finished: collect at CloudStorageOperations.scala:1862, took 22.533354 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.151941","level":"info","event":"25/07/18 07:59:00 INFO CloudStorageOperations$: Spark Connector Master: Finish uploading data for 12 partitions in 22.96 seconds.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.156412","level":"info","event":"25/07/18 07:59:00 INFO StageWriter$: writeToTableWithStagingTable: check table existence with \"RETAIL_SALES\".\"STAGING\".SILVER_DATA for SILVER_DATA","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.160124","level":"info","event":"25/07/18 07:59:00 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: desc table identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.349018","level":"info","event":"25/07/18 07:59:00 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: create   table if not exists identifier(?) (\"INVOICE_NO\" STRING ,\"STOCK_CODE\" STRING ,\"DESCRIPTION\" STRING ,\"QUANTITY\" INTEGER ,\"INVOICE_DATE\" STRING ,\"UNIT_PRICE\" DOUBLE ,\"CUSTOMER_ID\" STRING ,\"COUNTRY\" STRING ,\"REVENUE\" DOUBLE ,\"IS_RETURN\" INTEGER )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.653890","level":"info","event":"25/07/18 07:59:00 INFO StageWriter$: Begin to write at 2025-07-18T07:59:00.628429993 (Coordinated Universal Time)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.654473","level":"info","event":"25/07/18 07:59:00 INFO StageWriter$: Total file count is 12, non-empty files count is 12, total file size is 36.82 MB, total row count is 397.29 KB.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.659961","level":"info","event":"25/07/18 07:59:00 INFO StageWriter$: Now executing below command to write into table:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.660330","level":"info","event":"copy into SILVER_DATA_staging_1043620044 FROM @spark_connector_load_stage_hnRuArn3p7/iNloAmAaQm/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.660513","level":"info","event":"FILE_FORMAT = (","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.660605","level":"info","event":"    TYPE=CSV","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.660682","level":"info","event":"    FIELD_DELIMITER='|'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.660748","level":"info","event":"    NULL_IF=()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.660813","level":"info","event":"    FIELD_OPTIONALLY_ENCLOSED_BY='\"'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.660881","level":"info","event":"    TIMESTAMP_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.660944","level":"info","event":"    DATE_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.661004","level":"info","event":"    BINARY_FORMAT=BASE64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.661067","level":"info","event":"  )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.661135","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.661294","level":"info","event":"25/07/18 07:59:00 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: copy into SILVER_DATA_staging_1043620044 FROM @spark_connector_load_stage_hnRuArn3p7/iNloAmAaQm/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.661436","level":"info","event":"FILE_FORMAT = (","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.661513","level":"info","event":"    TYPE=CSV","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.661577","level":"info","event":"    FIELD_DELIMITER='|'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.661646","level":"info","event":"    NULL_IF=()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.661740","level":"info","event":"    FIELD_OPTIONALLY_ENCLOSED_BY='\"'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.661806","level":"info","event":"    TIMESTAMP_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.661866","level":"info","event":"    DATE_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.661924","level":"info","event":"    BINARY_FORMAT=BASE64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.662002","level":"info","event":"  )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.662094","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.834819","level":"info","event":"25/07/18 07:59:00 INFO SparkConnectorContext$: Spark connector register listener for: app-20250718075826-0001","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.837627","level":"info","event":"25/07/18 07:59:00 INFO SparkConnectorContext$: Add running query for app-20250718075826-0001 session: 182007829176382 queryId: 01bdc43f-0000-a176-0000-a5890001320e","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.842369","level":"info","event":"25/07/18 07:59:00 INFO StageWriter$: The query ID for async writing into table command is: 01bdc43f-0000-a176-0000-a5890001320e; The query ID URL is:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:00.843884","level":"info","event":"https://TRLIYPI-XK63730.snowflakecomputing.com/console#/monitoring/queries/detail?queryId=01bdc43f-0000-a176-0000-a5890001320e","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:05.962771","level":"info","event":"25/07/18 07:59:05 INFO SparkConnectorContext$: Remove running query for app-20250718075826-0001 session: 182007829176382 queryId: 01bdc43f-0000-a176-0000-a5890001320e","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:05.963468","level":"info","event":"25/07/18 07:59:05 INFO StageWriter$: First COPY command is done in 5.33 seconds at 2025-07-18T07:59:05.962052426, queryID is 01bdc43f-0000-a176-0000-a5890001320e","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:05.967960","level":"info","event":"25/07/18 07:59:05 INFO StageWriter$: Succeed to write in 5.34 seconds at 2025-07-18T07:59:05.966677930","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:05.969822","level":"info","event":"25/07/18 07:59:05 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: alter table identifier(?) swap with identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:06.155855","level":"info","event":"25/07/18 07:59:06 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: drop table identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:06.355732","level":"info","event":"25/07/18 07:59:06 INFO StageWriter$: Spark Connector Master: Total job time is 29.17 seconds including read & upload time: 22.97 seconds and COPY time: 6.20 seconds.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:06.360102","level":"info","event":"25/07/18 07:59:06 INFO ServerConnection$: Create ServerConnection with cached JDBC connection: 182007829176382","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:06.360549","level":"info","event":"25/07/18 07:59:06 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: select * from SILVER_DATA where 1 = 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.359939","level":"info","event":"25/07/18 07:59:07 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.361047","level":"info","event":"25/07/18 07:59:07 WARN SparkConnectorContext$: Finish cancelling all queries for app-20250718075826-0001","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.361193","level":"info","event":"25/07/18 07:59:07 INFO ServerConnection$: Close all 1 cached connection.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.370406","level":"info","event":"25/07/18 07:59:07 INFO SparkUI: Stopped Spark web UI at http://d22432c79974:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.372839","level":"info","event":"25/07/18 07:59:07 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.373180","level":"info","event":"25/07/18 07:59:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.633689","level":"info","event":"25/07/18 07:59:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.646028","level":"info","event":"25/07/18 07:59:07 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.646273","level":"info","event":"25/07/18 07:59:07 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.649738","level":"info","event":"25/07/18 07:59:07 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.652946","level":"info","event":"25/07/18 07:59:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.663115","level":"info","event":"25/07/18 07:59:07 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.663331","level":"info","event":"25/07/18 07:59:07 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.663738","level":"info","event":"25/07/18 07:59:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-56edea57-1468-41da-8503-b3df74acffab","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.668763","level":"info","event":"25/07/18 07:59:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-56edea57-1468-41da-8503-b3df74acffab/pyspark-b7aaa75a-85d1-4f78-93b2-40fb352e4137","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.671858","level":"info","event":"25/07/18 07:59:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-a1e1b0ae-bd07-43af-a49a-906801b0aea0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.719885","level":"info","event":"Command exited with return code 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T07:59:07.720754","level":"info","event":"Pushing xcom","ti":"RuntimeTaskInstance(id=UUID('01981c8a-5c6e-76c7-8b52-712e8d15ae6c'), task_id='silver_layer_snowflake', dag_id='retail_pipeline', run_id='manual__2025-07-18T07:58:06.424078+00:00', try_number=1, map_index=-1, hostname='cee2c2032bfa', context_carrier=None, task=<Task(BashOperator): silver_layer_snowflake>, bundle_instance=LocalDagBundle(name=dags-folder), max_tries=0, start_date=datetime.datetime(2025, 7, 18, 7, 58, 23, 677403, tzinfo=TzInfo(UTC)), end_date=None, is_mapped=False)","logger":"task"}
