{"timestamp":"2025-07-18T09:46:38.990504","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-18T09:46:38.991369","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/upload_to_hdfs_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-18T09:46:39.029715","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:39.030425","level":"info","event":"Running command: ['/usr/bin/bash', '-c', 'docker exec retail-spark-master                        /opt/bitnami/spark/bin/spark-submit                        --master spark://spark-master:7077 /opt/spark-apps/scripts/snowflake_staging.py']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:39.039865","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.257798","level":"info","event":"25/07/18 09:46:41 INFO SparkContext: Running Spark version 3.4.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.282586","level":"info","event":"25/07/18 09:46:41 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.282759","level":"info","event":"25/07/18 09:46:41 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.282836","level":"info","event":"25/07/18 09:46:41 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.283081","level":"info","event":"25/07/18 09:46:41 INFO SparkContext: Submitted application: Retail_Snowflake_Staging","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.300525","level":"info","event":"25/07/18 09:46:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.306964","level":"info","event":"25/07/18 09:46:41 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.307353","level":"info","event":"25/07/18 09:46:41 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.349338","level":"info","event":"25/07/18 09:46:41 INFO SecurityManager: Changing view acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.349500","level":"info","event":"25/07/18 09:46:41 INFO SecurityManager: Changing modify acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.349775","level":"info","event":"25/07/18 09:46:41 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.349989","level":"info","event":"25/07/18 09:46:41 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.350193","level":"info","event":"25/07/18 09:46:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.394425","level":"info","event":"25/07/18 09:46:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.596495","level":"info","event":"25/07/18 09:46:41 INFO Utils: Successfully started service 'sparkDriver' on port 44235.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.619872","level":"info","event":"25/07/18 09:46:41 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.651402","level":"info","event":"25/07/18 09:46:41 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.665828","level":"info","event":"25/07/18 09:46:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.666032","level":"info","event":"25/07/18 09:46:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.670490","level":"info","event":"25/07/18 09:46:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.688773","level":"info","event":"25/07/18 09:46:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-377d34ee-fc0b-4633-ae65-50ff6dc31fa2","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.702241","level":"info","event":"25/07/18 09:46:41 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.718145","level":"info","event":"25/07/18 09:46:41 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.822553","level":"info","event":"25/07/18 09:46:41 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.871465","level":"info","event":"25/07/18 09:46:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:41.981754","level":"info","event":"25/07/18 09:46:41 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://retail-spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.011574","level":"info","event":"25/07/18 09:46:42 INFO TransportClientFactory: Successfully created connection to retail-spark-master/172.24.0.4:7077 after 17 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.079163","level":"info","event":"25/07/18 09:46:42 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250718094642-0017","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.081310","level":"info","event":"25/07/18 09:46:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250718094642-0017/0 on worker-20250718090909-172.24.0.6-42597 (172.24.0.6:42597) with 12 core(s)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.083401","level":"info","event":"25/07/18 09:46:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20250718094642-0017/0 on hostPort 172.24.0.6:42597 with 12 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.084240","level":"info","event":"25/07/18 09:46:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45023.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.084401","level":"info","event":"25/07/18 09:46:42 INFO NettyBlockTransferService: Server created on 8da053d44e9f:45023","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.086304","level":"info","event":"25/07/18 09:46:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.091027","level":"info","event":"25/07/18 09:46:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8da053d44e9f, 45023, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.094161","level":"info","event":"25/07/18 09:46:42 INFO BlockManagerMasterEndpoint: Registering block manager 8da053d44e9f:45023 with 434.4 MiB RAM, BlockManagerId(driver, 8da053d44e9f, 45023, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.096394","level":"info","event":"25/07/18 09:46:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8da053d44e9f, 45023, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.098059","level":"info","event":"25/07/18 09:46:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8da053d44e9f, 45023, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.108420","level":"info","event":"25/07/18 09:46:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250718094642-0017/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.255061","level":"info","event":"25/07/18 09:46:42 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.485537","level":"info","event":"25/07/18 09:46:42 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:42.492886","level":"info","event":"25/07/18 09:46:42 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:43.863260","level":"info","event":"25/07/18 09:46:43 INFO InMemoryFileIndex: It took 100 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.210761","level":"info","event":"25/07/18 09:46:44 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.24.0.6:47564) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.287368","level":"info","event":"25/07/18 09:46:44 INFO BlockManagerMasterEndpoint: Registering block manager 172.24.0.6:41647 with 434.4 MiB RAM, BlockManagerId(0, 172.24.0.6, 41647, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.315166","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.315372","level":"info","event":"  File \"/opt/spark-apps/scripts/snowflake_staging.py\", line 26, in <module>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.317219","level":"info","event":"    df = spark.read.csv(\"hdfs://namenode:8020/data/silver/retail_cleaned.csv\", header=True, inferSchema=schema)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.317342","level":"info","event":"  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 727, in csv","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.317896","level":"info","event":"  File \"/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.318513","level":"info","event":"  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 169, in deco","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.319457","level":"info","event":"  File \"/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321108","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o28.csv.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321295","level":"info","event":": org.apache.spark.SparkException: inferSchema flag can be true or false.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321375","level":"info","event":"\tat org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1520)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321429","level":"info","event":"\tat org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:99)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321463","level":"info","event":"\tat org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:122)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321487","level":"info","event":"\tat org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:50)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321511","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321535","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321557","level":"info","event":"\tat scala.Option.orElse(Option.scala:447)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321583","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321605","level":"info","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321627","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321653","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321673","level":"info","event":"\tat scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321692","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321712","level":"info","event":"\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321732","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321751","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321772","level":"info","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321792","level":"info","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321816","level":"info","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321848","level":"info","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321885","level":"info","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321919","level":"info","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321956","level":"info","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.321996","level":"info","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.322037","level":"info","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.322065","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:833)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.322092","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.345145","level":"info","event":"25/07/18 09:46:44 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.345327","level":"info","event":"25/07/18 09:46:44 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.353996","level":"info","event":"25/07/18 09:46:44 INFO SparkUI: Stopped Spark web UI at http://8da053d44e9f:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.357434","level":"info","event":"25/07/18 09:46:44 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.357650","level":"info","event":"25/07/18 09:46:44 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.370657","level":"info","event":"25/07/18 09:46:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.388292","level":"info","event":"25/07/18 09:46:44 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.389806","level":"info","event":"25/07/18 09:46:44 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.394247","level":"info","event":"25/07/18 09:46:44 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.396051","level":"info","event":"25/07/18 09:46:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.405219","level":"info","event":"25/07/18 09:46:44 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.405421","level":"info","event":"25/07/18 09:46:44 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.406478","level":"info","event":"25/07/18 09:46:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-52cf66ff-61be-48fb-8a8f-0eee134352b8","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.410363","level":"info","event":"25/07/18 09:46:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7b3ec67-5c5c-4897-916d-7516e162ba03/pyspark-de33d656-c45e-4cec-b8b7-70cf0bf231de","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.413953","level":"info","event":"25/07/18 09:46:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7b3ec67-5c5c-4897-916d-7516e162ba03","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.452513","level":"info","event":"Command exited with return code 1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-18T09:46:44.453011","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Bash command failed. The command returned a non-zero exit code 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/bash.py","lineno":233,"name":"execute"}]}]}
