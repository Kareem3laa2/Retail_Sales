{"timestamp":"2025-07-20T04:22:42.215731","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-20T04:22:42.216418","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/upload_to_hdfs_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-20T04:22:42.241273","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:42.242181","level":"info","event":"Running command: ['/usr/bin/bash', '-c', 'docker exec retail-spark-master                        /opt/bitnami/spark/bin/spark-submit                        --master spark://spark-master:7077 /opt/spark-apps/scripts/snowflake_staging.py']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:42.249795","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.323112","level":"info","event":"25/07/20 04:22:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.819875","level":"info","event":"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.827504","level":"info","event":"25/07/20 04:22:43 INFO SparkContext: Running Spark version 3.1.1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.866401","level":"info","event":"25/07/20 04:22:43 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.866570","level":"info","event":"25/07/20 04:22:43 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.866647","level":"info","event":"25/07/20 04:22:43 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.867323","level":"info","event":"25/07/20 04:22:43 INFO SparkContext: Submitted application: Retail_Snowflake_Staging","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.889091","level":"info","event":"25/07/20 04:22:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.897593","level":"info","event":"25/07/20 04:22:43 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.897749","level":"info","event":"25/07/20 04:22:43 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.942277","level":"info","event":"25/07/20 04:22:43 INFO SecurityManager: Changing view acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.942436","level":"info","event":"25/07/20 04:22:43 INFO SecurityManager: Changing modify acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.942512","level":"info","event":"25/07/20 04:22:43 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.942579","level":"info","event":"25/07/20 04:22:43 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:43.942638","level":"info","event":"25/07/20 04:22:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.112400","level":"info","event":"25/07/20 04:22:44 INFO Utils: Successfully started service 'sparkDriver' on port 45601.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.140231","level":"info","event":"25/07/20 04:22:44 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.165074","level":"info","event":"25/07/20 04:22:44 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.189575","level":"info","event":"25/07/20 04:22:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.190058","level":"info","event":"25/07/20 04:22:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.193624","level":"info","event":"25/07/20 04:22:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.204670","level":"info","event":"25/07/20 04:22:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3e14048a-45ee-4510-bf8e-204a0750bff8","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.220519","level":"info","event":"25/07/20 04:22:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.234346","level":"info","event":"25/07/20 04:22:44 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.407345","level":"info","event":"25/07/20 04:22:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.448255","level":"info","event":"25/07/20 04:22:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://40b3ac35ea26:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.586164","level":"info","event":"25/07/20 04:22:44 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://retail-spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.624719","level":"info","event":"25/07/20 04:22:44 INFO TransportClientFactory: Successfully created connection to retail-spark-master/172.24.0.3:7077 after 23 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.693386","level":"info","event":"25/07/20 04:22:44 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250720042244-0001","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.694544","level":"info","event":"25/07/20 04:22:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250720042244-0001/0 on worker-20250720041936-172.24.0.5-33199 (172.24.0.5:33199) with 12 core(s)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.695615","level":"info","event":"25/07/20 04:22:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20250720042244-0001/0 on hostPort 172.24.0.5:33199 with 12 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.701718","level":"info","event":"25/07/20 04:22:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37785.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.701888","level":"info","event":"25/07/20 04:22:44 INFO NettyBlockTransferService: Server created on 40b3ac35ea26:37785","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.703006","level":"info","event":"25/07/20 04:22:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.709588","level":"info","event":"25/07/20 04:22:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 40b3ac35ea26, 37785, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.712779","level":"info","event":"25/07/20 04:22:44 INFO BlockManagerMasterEndpoint: Registering block manager 40b3ac35ea26:37785 with 366.3 MiB RAM, BlockManagerId(driver, 40b3ac35ea26, 37785, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.714924","level":"info","event":"25/07/20 04:22:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 40b3ac35ea26, 37785, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.716253","level":"info","event":"25/07/20 04:22:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 40b3ac35ea26, 37785, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.719081","level":"info","event":"25/07/20 04:22:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250720042244-0001/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:44.864036","level":"info","event":"25/07/20 04:22:44 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:45.093288","level":"info","event":"25/07/20 04:22:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/bitnami/spark/spark-warehouse').","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:45.093428","level":"info","event":"25/07/20 04:22:45 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:46.191459","level":"info","event":"25/07/20 04:22:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.24.0.5:43418) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:46.280129","level":"info","event":"25/07/20 04:22:46 INFO BlockManagerMasterEndpoint: Registering block manager 172.24.0.5:41741 with 366.3 MiB RAM, BlockManagerId(0, 172.24.0.5, 41741, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:46.300297","level":"info","event":"25/07/20 04:22:46 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:47.917216","level":"info","event":"25/07/20 04:22:47 WARN SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.1.1 with a connector designed to support Spark 3.3. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:47.978304","level":"info","event":"25/07/20 04:22:47 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:47.979315","level":"info","event":"25/07/20 04:22:47 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:47.982763","level":"info","event":"25/07/20 04:22:47 INFO FileSourceStrategy: Output Data Schema: struct<invoice_no: string, stock_code: string, description: string, quantity: int, invoice_date: string ... 8 more fields>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:48.065899","level":"info","event":"25/07/20 04:22:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:48.110199","level":"info","event":"25/07/20 04:22:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:48.112490","level":"info","event":"25/07/20 04:22:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 40b3ac35ea26:37785 (size: 27.3 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:48.114509","level":"info","event":"25/07/20 04:22:48 INFO SparkContext: Created broadcast 0 from rdd at SnowflakeWriter.scala:87","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:48.130630","level":"info","event":"25/07/20 04:22:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7576840 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:48.302865","level":"info","event":"25/07/20 04:22:48 WARN ServerConnection$: JDBC 3.13.31 is being used. But the certified JDBC version 3.13.30 is recommended.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.630367","level":"info","event":"25/07/20 04:22:50 INFO ServerConnection$: Create ServerConnection with new JDBC connection: 182007829200906","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.647824","level":"info","event":"25/07/20 04:22:50 INFO SparkConnectorContext$: Spark Connector system config: {","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648001","level":"info","event":"  \"spark_connector_version\" : \"2.12.0\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648109","level":"info","event":"  \"spark_version\" : \"3.1.1\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648185","level":"info","event":"  \"application_name\" : \"Retail_Snowflake_Staging\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648272","level":"info","event":"  \"scala_version\" : \"2.12.10\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648318","level":"info","event":"  \"java_version\" : \"1.8.0_292\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648356","level":"info","event":"  \"jdbc_version\" : \"3.13.31\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648390","level":"info","event":"  \"certified_jdbc_version\" : \"3.13.30\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648426","level":"info","event":"  \"os_name\" : \"Linux\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648459","level":"info","event":"  \"max_memory_in_mb\" : 910,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648491","level":"info","event":"  \"total_memory_in_mb\" : 613,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648521","level":"info","event":"  \"free_memory_in_mb\" : 544,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648550","level":"info","event":"  \"cpu_cores\" : 12,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648615","level":"info","event":"  \"spark_application_id\" : \"app-20250720042244-0001\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648658","level":"info","event":"  \"spark_language\" : \"Scala\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648694","level":"info","event":"  \"is_pyspark\" : false,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648731","level":"info","event":"  \"spark_config\" : {","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648763","level":"info","event":"    \"spark.rdd.compress\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648794","level":"info","event":"    \"spark.sql.warehouse.dir\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648824","level":"info","event":"    \"spark.app.name\" : \"Retail_Snowflake_Staging\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648855","level":"info","event":"    \"spark.app.id\" : \"app-20250720042244-0001\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648917","level":"info","event":"    \"spark.driver.port\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.648953","level":"info","event":"    \"spark.driver.host\" : \"40b3ac35ea26\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.649070","level":"info","event":"    \"spark.submit.pyFiles\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.649163","level":"info","event":"    \"spark.executor.id\" : \"driver\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.649230","level":"info","event":"    \"spark.app.startTime\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.649270","level":"info","event":"    \"spark.master\" : \"spark://retail-spark-master:7077\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.649305","level":"info","event":"    \"spark.submit.deployMode\" : \"client\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.649338","level":"info","event":"    \"spark.serializer.objectStreamReset\" : \"N/A\"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.649809","level":"info","event":"  },","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.649875","level":"info","event":"  \"libraries\" : [ \"py4j\", \"py4j.commands\", \"py4j.reflection\", \"sun.reflect\", \"org.apache.spark.rdd\" ],","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.649912","level":"info","event":"  \"dependencies\" : [ ],","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.649973","level":"info","event":"  \"cluster_node_count\" : 2,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.650026","level":"info","event":"  \"spark_default_parallelism\" : 12,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.650073","level":"info","event":"  \"deploy_mode\" : \"client\"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.650122","level":"info","event":"}","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:50.661024","level":"info","event":"25/07/20 04:22:50 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: alter session set timezone = 'Etc/UTC' , timestamp_ntz_output_format = 'YYYY-MM-DD HH24:MI:SS.FF3', timestamp_ltz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3', timestamp_tz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3' ;","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:51.264493","level":"info","event":"25/07/20 04:22:51 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: create  temporary stage if not exists identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.246361","level":"info","event":"25/07/20 04:22:52 INFO CloudStorageOperations$: Spark Connector Master: Begin to process and upload data for 11 partitions: directory=vOuVsdYW3c CSV true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.246481","level":"info","event":"25/07/20 04:22:52 INFO CloudStorageOperations$: Spark Connector Master: Begin to retrieve pre-signed URL or down-scoped token for 11 files by calling PUT command.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.609569","level":"info","event":"25/07/20 04:22:52 INFO CloudStorageOperations$: Spark Connector Master: Upload file to GCP with down-scoped token instead of pre-signed URL.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.612893","level":"info","event":"25/07/20 04:22:52 INFO CloudStorageOperations$: Spark Connector Master: Time to retrieve down-scoped token for 1/11 files is 363 ms.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.743682","level":"info","event":"25/07/20 04:22:52 INFO SparkContext: Starting job: collect at CloudStorageOperations.scala:1862","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.757451","level":"info","event":"25/07/20 04:22:52 INFO DAGScheduler: Got job 0 (collect at CloudStorageOperations.scala:1862) with 11 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.757605","level":"info","event":"25/07/20 04:22:52 INFO DAGScheduler: Final stage: ResultStage 0 (collect at CloudStorageOperations.scala:1862)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.757754","level":"info","event":"25/07/20 04:22:52 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.759484","level":"info","event":"25/07/20 04:22:52 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.763736","level":"info","event":"25/07/20 04:22:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at mapPartitionsWithIndex at CloudStorageOperations.scala:1839), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.783613","level":"info","event":"25/07/20 04:22:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 23.2 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.787506","level":"info","event":"25/07/20 04:22:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 13.8 KiB, free 365.9 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.787724","level":"info","event":"25/07/20 04:22:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 40b3ac35ea26:37785 (size: 13.8 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.787925","level":"info","event":"25/07/20 04:22:52 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.803708","level":"info","event":"25/07/20 04:22:52 INFO DAGScheduler: Submitting 11 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at mapPartitionsWithIndex at CloudStorageOperations.scala:1839) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.804360","level":"info","event":"25/07/20 04:22:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 11 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.838554","level":"info","event":"25/07/20 04:22:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.24.0.5, executor 0, partition 0, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.843403","level":"info","event":"25/07/20 04:22:52 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.24.0.5, executor 0, partition 1, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.843775","level":"info","event":"25/07/20 04:22:52 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.24.0.5, executor 0, partition 2, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.843967","level":"info","event":"25/07/20 04:22:52 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.24.0.5, executor 0, partition 3, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.844274","level":"info","event":"25/07/20 04:22:52 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.24.0.5, executor 0, partition 4, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.844831","level":"info","event":"25/07/20 04:22:52 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.24.0.5, executor 0, partition 5, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.845099","level":"info","event":"25/07/20 04:22:52 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.24.0.5, executor 0, partition 6, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.845279","level":"info","event":"25/07/20 04:22:52 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.24.0.5, executor 0, partition 7, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.845821","level":"info","event":"25/07/20 04:22:52 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.24.0.5, executor 0, partition 8, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.845931","level":"info","event":"25/07/20 04:22:52 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.24.0.5, executor 0, partition 9, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:52.845994","level":"info","event":"25/07/20 04:22:52 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.24.0.5, executor 0, partition 10, ANY, 5075 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:53.115565","level":"info","event":"25/07/20 04:22:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.24.0.5:41741 (size: 13.8 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.533943","level":"info","event":"25/07/20 04:22:55 ERROR Inbox: Ignoring error","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534081","level":"info","event":"java.io.NotSerializableException: org.apache.spark.storage.StorageStatus","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534123","level":"info","event":"Serialization stack:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534177","level":"info","event":"\t- object not serializable (class: org.apache.spark.storage.StorageStatus, value: org.apache.spark.storage.StorageStatus@b59ae55)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534211","level":"info","event":"\t- element of array (index: 0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534244","level":"info","event":"\t- array (class [Lorg.apache.spark.storage.StorageStatus;, size 2)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534268","level":"info","event":"\tat org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534294","level":"info","event":"\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534323","level":"info","event":"\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534344","level":"info","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:286)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534366","level":"info","event":"\tat org.apache.spark.rpc.netty.RemoteNettyRpcCallContext.send(NettyRpcCallContext.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534396","level":"info","event":"\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534417","level":"info","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:147)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534438","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534468","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534489","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534509","level":"info","event":"\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534534","level":"info","event":"\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534559","level":"info","event":"\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534579","level":"info","event":"\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534600","level":"info","event":"\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534627","level":"info","event":"\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.534648","level":"info","event":"\tat java.lang.Thread.run(Thread.java:748)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:22:55.627765","level":"info","event":"25/07/20 04:22:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.24.0.5:41741 (size: 27.3 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:09.287909","level":"info","event":"25/07/20 04:23:09 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 16441 ms on 172.24.0.5 (executor 0) (1/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:09.529004","level":"info","event":"25/07/20 04:23:09 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 16683 ms on 172.24.0.5 (executor 0) (2/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:18.514269","level":"info","event":"25/07/20 04:23:18 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 25668 ms on 172.24.0.5 (executor 0) (3/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:18.832392","level":"info","event":"25/07/20 04:23:18 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 25988 ms on 172.24.0.5 (executor 0) (4/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:19.315632","level":"info","event":"25/07/20 04:23:19 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 26464 ms on 172.24.0.5 (executor 0) (5/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:19.862081","level":"info","event":"25/07/20 04:23:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 27033 ms on 172.24.0.5 (executor 0) (6/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.002872","level":"info","event":"25/07/20 04:23:20 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 27158 ms on 172.24.0.5 (executor 0) (7/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.205058","level":"info","event":"25/07/20 04:23:20 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 27357 ms on 172.24.0.5 (executor 0) (8/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.407526","level":"info","event":"25/07/20 04:23:20 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 27557 ms on 172.24.0.5 (executor 0) (9/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.423370","level":"info","event":"25/07/20 04:23:20 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 27579 ms on 172.24.0.5 (executor 0) (10/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.537151","level":"info","event":"25/07/20 04:23:20 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 27692 ms on 172.24.0.5 (executor 0) (11/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.540453","level":"info","event":"25/07/20 04:23:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.542662","level":"info","event":"25/07/20 04:23:20 INFO DAGScheduler: ResultStage 0 (collect at CloudStorageOperations.scala:1862) finished in 27.757 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.551063","level":"info","event":"25/07/20 04:23:20 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.551872","level":"info","event":"25/07/20 04:23:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.554751","level":"info","event":"25/07/20 04:23:20 INFO DAGScheduler: Job 0 finished: collect at CloudStorageOperations.scala:1862, took 24.085805 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.558650","level":"info","event":"25/07/20 04:23:20 INFO CloudStorageOperations$: Spark Connector Master: Finish uploading data for 11 partitions in 28.32 seconds.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.559768","level":"info","event":"25/07/20 04:23:20 INFO StageWriter$: writeToTableWithStagingTable: check table existence with \"RETAIL_SALES\".\"STAGING\".SILVER_DATA for SILVER_DATA","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.560774","level":"info","event":"25/07/20 04:23:20 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: desc table identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.699675","level":"info","event":"25/07/20 04:23:20 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: create   table if not exists identifier(?) (\"INVOICE_NO\" STRING ,\"STOCK_CODE\" STRING ,\"DESCRIPTION\" STRING ,\"QUANTITY\" INTEGER ,\"INVOICE_DATE\" STRING ,\"UNIT_PRICE\" DOUBLE ,\"CUSTOMER_ID\" STRING ,\"COUNTRY\" STRING ,\"REVENUE\" DOUBLE ,\"IS_RETURN\" INTEGER )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.962321","level":"info","event":"25/07/20 04:23:20 INFO StageWriter$: Begin to write at 2025-07-20T04:23:20.950 (Coordinated Universal Time)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.964628","level":"info","event":"25/07/20 04:23:20 INFO StageWriter$: Total file count is 11, non-empty files count is 11, total file size is 43.36 MB, total row count is 397.28 KB.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.970428","level":"info","event":"25/07/20 04:23:20 INFO StageWriter$: Now executing below command to write into table:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.970570","level":"info","event":"copy into SILVER_DATA_staging_647062767 FROM @spark_connector_load_stage_1mvEdFtYSS/vOuVsdYW3c/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.970612","level":"info","event":"FILE_FORMAT = (","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.970641","level":"info","event":"    TYPE=CSV","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.970668","level":"info","event":"    FIELD_DELIMITER='|'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.970693","level":"info","event":"    NULL_IF=()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.970717","level":"info","event":"    FIELD_OPTIONALLY_ENCLOSED_BY='\"'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.970804","level":"info","event":"    TIMESTAMP_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.970919","level":"info","event":"    DATE_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.971002","level":"info","event":"    BINARY_FORMAT=BASE64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.971092","level":"info","event":"  )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.971186","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.971299","level":"info","event":"25/07/20 04:23:20 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: copy into SILVER_DATA_staging_647062767 FROM @spark_connector_load_stage_1mvEdFtYSS/vOuVsdYW3c/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.971434","level":"info","event":"FILE_FORMAT = (","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.971530","level":"info","event":"    TYPE=CSV","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.971604","level":"info","event":"    FIELD_DELIMITER='|'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.971669","level":"info","event":"    NULL_IF=()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.971772","level":"info","event":"    FIELD_OPTIONALLY_ENCLOSED_BY='\"'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.971882","level":"info","event":"    TIMESTAMP_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.971991","level":"info","event":"    DATE_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.972095","level":"info","event":"    BINARY_FORMAT=BASE64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.972169","level":"info","event":"  )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:20.972237","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:21.115535","level":"info","event":"25/07/20 04:23:21 INFO SparkConnectorContext$: Spark connector register listener for: app-20250720042244-0001","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:21.116159","level":"info","event":"25/07/20 04:23:21 INFO SparkConnectorContext$: Add running query for app-20250720042244-0001 session: 182007829200906 queryId: 01bdcea7-0000-a19b-0000-a58900017bf2","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:21.117488","level":"info","event":"25/07/20 04:23:21 INFO StageWriter$: The query ID for async writing into table command is: 01bdcea7-0000-a19b-0000-a58900017bf2; The query ID URL is:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:21.117616","level":"info","event":"https://TRLIYPI-XK63730.snowflakecomputing.com/console#/monitoring/queries/detail?queryId=01bdcea7-0000-a19b-0000-a58900017bf2","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:26.096657","level":"info","event":"25/07/20 04:23:26 INFO SparkConnectorContext$: Remove running query for app-20250720042244-0001 session: 182007829200906 queryId: 01bdcea7-0000-a19b-0000-a58900017bf2","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:26.097279","level":"info","event":"25/07/20 04:23:26 INFO StageWriter$: First COPY command is done in 5.15 seconds at 2025-07-20T04:23:26.096, queryID is 01bdcea7-0000-a19b-0000-a58900017bf2","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:26.104437","level":"info","event":"25/07/20 04:23:26 INFO StageWriter$: Succeed to write in 5.15 seconds at 2025-07-20T04:23:26.103","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:26.105587","level":"info","event":"25/07/20 04:23:26 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: alter table identifier(?) swap with identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:26.266512","level":"info","event":"25/07/20 04:23:26 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: drop table identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:26.450168","level":"info","event":"25/07/20 04:23:26 INFO StageWriter$: Spark Connector Master: Total job time is 34.21 seconds including read & upload time: 28.32 seconds and COPY time: 5.89 seconds.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:26.450379","level":"info","event":"25/07/20 04:23:26 WARN SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.1.1 with a connector designed to support Spark 3.3. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:26.523453","level":"info","event":"25/07/20 04:23:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 40b3ac35ea26:37785 in memory (size: 13.8 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:26.536592","level":"info","event":"25/07/20 04:23:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.24.0.5:41741 in memory (size: 13.8 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.461733","level":"info","event":"25/07/20 04:23:27 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.467218","level":"info","event":"25/07/20 04:23:27 WARN SparkConnectorContext$: Finish cancelling all queries for app-20250720042244-0001","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.469135","level":"info","event":"25/07/20 04:23:27 INFO ServerConnection$: Close all 1 cached connection.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.485709","level":"info","event":"25/07/20 04:23:27 INFO SparkUI: Stopped Spark web UI at http://40b3ac35ea26:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.489903","level":"info","event":"25/07/20 04:23:27 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.490035","level":"info","event":"25/07/20 04:23:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.745964","level":"info","event":"25/07/20 04:23:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.772545","level":"info","event":"25/07/20 04:23:27 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.772700","level":"info","event":"25/07/20 04:23:27 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.776156","level":"info","event":"25/07/20 04:23:27 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.779093","level":"info","event":"25/07/20 04:23:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.789400","level":"info","event":"25/07/20 04:23:27 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.789632","level":"info","event":"25/07/20 04:23:27 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.790102","level":"info","event":"25/07/20 04:23:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-22816e39-46b7-411e-8ce2-a8d4009bb1e0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.793532","level":"info","event":"25/07/20 04:23:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-9780fadc-94f0-465d-b98f-71a3ec5170d3","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.797269","level":"info","event":"25/07/20 04:23:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-22816e39-46b7-411e-8ce2-a8d4009bb1e0/pyspark-225e1647-8809-4e34-a448-c506625b8b44","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.862990","level":"info","event":"Command exited with return code 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:23:27.864078","level":"info","event":"Pushing xcom","ti":"RuntimeTaskInstance(id=UUID('01982611-907c-7221-8cae-299f2c07ea1f'), task_id='silver_layer_snowflake', dag_id='retail_pipeline', run_id='manual__2025-07-20T04:22:22.054611+00:00', try_number=1, map_index=-1, hostname='52b07f18d859', context_carrier=None, task=<Task(BashOperator): silver_layer_snowflake>, bundle_instance=LocalDagBundle(name=dags-folder), max_tries=0, start_date=datetime.datetime(2025, 7, 20, 4, 22, 42, 177051, tzinfo=TzInfo(UTC)), end_date=None, is_mapped=False)","logger":"task"}
