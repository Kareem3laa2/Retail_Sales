{"timestamp":"2025-07-20T04:12:20.319481","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-20T04:12:20.319978","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/upload_to_hdfs_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-20T04:12:20.333712","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:20.334328","level":"info","event":"Running command: ['/usr/bin/bash', '-c', 'docker exec retail-spark-master                        /opt/bitnami/spark/bin/spark-submit                        --master spark://spark-master:7077 /opt/spark-apps/scripts/snowflake_staging.py']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:20.344156","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.401131","level":"info","event":"25/07/20 04:12:22 INFO SparkContext: Running Spark version 3.4.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.426432","level":"info","event":"25/07/20 04:12:22 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.426627","level":"info","event":"25/07/20 04:12:22 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.427139","level":"info","event":"25/07/20 04:12:22 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.427594","level":"info","event":"25/07/20 04:12:22 INFO SparkContext: Submitted application: Retail_Snowflake_Staging","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.445645","level":"info","event":"25/07/20 04:12:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.452232","level":"info","event":"25/07/20 04:12:22 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.452430","level":"info","event":"25/07/20 04:12:22 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.491420","level":"info","event":"25/07/20 04:12:22 INFO SecurityManager: Changing view acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.491659","level":"info","event":"25/07/20 04:12:22 INFO SecurityManager: Changing modify acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.491724","level":"info","event":"25/07/20 04:12:22 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.492242","level":"info","event":"25/07/20 04:12:22 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.492507","level":"info","event":"25/07/20 04:12:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.543086","level":"info","event":"25/07/20 04:12:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.730761","level":"info","event":"25/07/20 04:12:22 INFO Utils: Successfully started service 'sparkDriver' on port 40945.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.749987","level":"info","event":"25/07/20 04:12:22 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.781096","level":"info","event":"25/07/20 04:12:22 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.794211","level":"info","event":"25/07/20 04:12:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.794970","level":"info","event":"25/07/20 04:12:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.799598","level":"info","event":"25/07/20 04:12:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.820350","level":"info","event":"25/07/20 04:12:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9360a40c-fb2f-4b5d-a9a9-e2444762c9bd","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.834565","level":"info","event":"25/07/20 04:12:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.853540","level":"info","event":"25/07/20 04:12:22 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.945047","level":"info","event":"25/07/20 04:12:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:22.988234","level":"info","event":"25/07/20 04:12:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.081992","level":"info","event":"25/07/20 04:12:23 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://retail-spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.122419","level":"info","event":"25/07/20 04:12:23 INFO TransportClientFactory: Successfully created connection to retail-spark-master/172.24.0.3:7077 after 25 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.197355","level":"info","event":"25/07/20 04:12:23 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250720041223-0001","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.200566","level":"info","event":"25/07/20 04:12:23 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250720041223-0001/0 on worker-20250720040929-172.24.0.5-46693 (172.24.0.5:46693) with 12 core(s)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.202173","level":"info","event":"25/07/20 04:12:23 INFO StandaloneSchedulerBackend: Granted executor ID app-20250720041223-0001/0 on hostPort 172.24.0.5:46693 with 12 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.203521","level":"info","event":"25/07/20 04:12:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46785.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.203686","level":"info","event":"25/07/20 04:12:23 INFO NettyBlockTransferService: Server created on 8da053d44e9f:46785","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.205123","level":"info","event":"25/07/20 04:12:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.209797","level":"info","event":"25/07/20 04:12:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8da053d44e9f, 46785, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.213124","level":"info","event":"25/07/20 04:12:23 INFO BlockManagerMasterEndpoint: Registering block manager 8da053d44e9f:46785 with 434.4 MiB RAM, BlockManagerId(driver, 8da053d44e9f, 46785, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.215494","level":"info","event":"25/07/20 04:12:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8da053d44e9f, 46785, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.216879","level":"info","event":"25/07/20 04:12:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8da053d44e9f, 46785, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.230207","level":"info","event":"25/07/20 04:12:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250720041223-0001/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.380456","level":"info","event":"25/07/20 04:12:23 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.593092","level":"info","event":"25/07/20 04:12:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:23.600939","level":"info","event":"25/07/20 04:12:23 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:24.809585","level":"info","event":"25/07/20 04:12:24 INFO InMemoryFileIndex: It took 70 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:25.057348","level":"info","event":"25/07/20 04:12:25 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.24.0.5:44842) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:25.137869","level":"info","event":"25/07/20 04:12:25 INFO BlockManagerMasterEndpoint: Registering block manager 172.24.0.5:43519 with 434.4 MiB RAM, BlockManagerId(0, 172.24.0.5, 43519, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:26.356106","level":"info","event":"25/07/20 04:12:26 WARN SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.4.0 with a connector designed to support Spark 3.3. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:26.402141","level":"info","event":"25/07/20 04:12:26 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:26.409053","level":"info","event":"25/07/20 04:12:26 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:26.585142","level":"info","event":"25/07/20 04:12:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.3 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:26.627012","level":"info","event":"25/07/20 04:12:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 434.2 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:26.629946","level":"info","event":"25/07/20 04:12:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8da053d44e9f:46785 (size: 34.1 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:26.632944","level":"info","event":"25/07/20 04:12:26 INFO SparkContext: Created broadcast 0 from rdd at SnowflakeWriter.scala:87","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:26.648137","level":"info","event":"25/07/20 04:12:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7576840 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:26.756449","level":"info","event":"25/07/20 04:12:26 WARN ServerConnection$: JDBC 3.13.31 is being used. But the certified JDBC version 3.13.30 is recommended.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.954650","level":"info","event":"25/07/20 04:12:28 INFO ServerConnection$: Create ServerConnection with new JDBC connection: 182007829200902","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.974516","level":"info","event":"25/07/20 04:12:28 INFO SparkConnectorContext$: Spark Connector system config: {","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.974663","level":"info","event":"  \"spark_connector_version\" : \"2.12.0\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.974738","level":"info","event":"  \"spark_version\" : \"3.4.0\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.974801","level":"info","event":"  \"application_name\" : \"Retail_Snowflake_Staging\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.974854","level":"info","event":"  \"scala_version\" : \"2.12.17\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.974906","level":"info","event":"  \"java_version\" : \"17.0.7\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.974959","level":"info","event":"  \"jdbc_version\" : \"3.13.31\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975011","level":"info","event":"  \"certified_jdbc_version\" : \"3.13.30\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975078","level":"info","event":"  \"os_name\" : \"Linux\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975132","level":"info","event":"  \"max_memory_in_mb\" : 1024,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975192","level":"info","event":"  \"total_memory_in_mb\" : 208,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975250","level":"info","event":"  \"free_memory_in_mb\" : 109,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975299","level":"info","event":"  \"cpu_cores\" : 12,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975352","level":"info","event":"  \"spark_application_id\" : \"app-20250720041223-0001\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975431","level":"info","event":"  \"spark_language\" : \"Scala\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975505","level":"info","event":"  \"is_pyspark\" : false,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975568","level":"info","event":"  \"spark_config\" : {","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975625","level":"info","event":"    \"spark.sql.warehouse.dir\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975672","level":"info","event":"    \"spark.app.name\" : \"Retail_Snowflake_Staging\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975716","level":"info","event":"    \"spark.executor.id\" : \"driver\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975760","level":"info","event":"    \"spark.driver.host\" : \"8da053d44e9f\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975806","level":"info","event":"    \"spark.app.submitTime\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975847","level":"info","event":"    \"spark.app.startTime\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975907","level":"info","event":"    \"spark.driver.extraJavaOptions\" : \"-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false --add-exports java.base/sun.nio.ch=ALL-UNNAMED\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.975992","level":"info","event":"    \"spark.rdd.compress\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976056","level":"info","event":"    \"spark.serializer.objectStreamReset\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976120","level":"info","event":"    \"spark.submit.pyFiles\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976192","level":"info","event":"    \"spark.driver.port\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976262","level":"info","event":"    \"spark.submit.deployMode\" : \"client\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976312","level":"info","event":"    \"spark.master\" : \"spark://retail-spark-master:7077\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976358","level":"info","event":"    \"spark.app.id\" : \"app-20250720041223-0001\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976408","level":"info","event":"    \"spark.executor.extraJavaOptions\" : \"-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976459","level":"info","event":"  },","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976505","level":"info","event":"  \"libraries\" : [ \"py4j\", \"py4j.commands\", \"py4j.reflection\" ],","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976570","level":"info","event":"  \"dependencies\" : [ ],","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976624","level":"info","event":"  \"cluster_node_count\" : 2,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976669","level":"info","event":"  \"spark_default_parallelism\" : 12,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976713","level":"info","event":"  \"deploy_mode\" : \"client\"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.976760","level":"info","event":"}","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:28.986255","level":"info","event":"25/07/20 04:12:28 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: alter session set timezone = 'Etc/UTC' , timestamp_ntz_output_format = 'YYYY-MM-DD HH24:MI:SS.FF3', timestamp_ltz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3', timestamp_tz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3' ;","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:29.736849","level":"info","event":"25/07/20 04:12:29 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: create  temporary stage if not exists identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:30.834341","level":"info","event":"25/07/20 04:12:30 INFO CloudStorageOperations$: Spark Connector Master: Begin to process and upload data for 11 partitions: directory=qWy9khrrUL CSV true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:30.834568","level":"info","event":"25/07/20 04:12:30 INFO CloudStorageOperations$: Spark Connector Master: Begin to retrieve pre-signed URL or down-scoped token for 11 files by calling PUT command.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.249249","level":"info","event":"25/07/20 04:12:31 INFO CloudStorageOperations$: Spark Connector Master: Upload file to GCP with down-scoped token instead of pre-signed URL.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.251242","level":"info","event":"25/07/20 04:12:31 INFO CloudStorageOperations$: Spark Connector Master: Time to retrieve down-scoped token for 1/11 files is 415 ms.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.359470","level":"info","event":"25/07/20 04:12:31 INFO SparkContext: Starting job: collect at CloudStorageOperations.scala:1862","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.374674","level":"info","event":"25/07/20 04:12:31 INFO DAGScheduler: Got job 0 (collect at CloudStorageOperations.scala:1862) with 11 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.375068","level":"info","event":"25/07/20 04:12:31 INFO DAGScheduler: Final stage: ResultStage 0 (collect at CloudStorageOperations.scala:1862)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.375844","level":"info","event":"25/07/20 04:12:31 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.378357","level":"info","event":"25/07/20 04:12:31 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.381320","level":"info","event":"25/07/20 04:12:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at mapPartitionsWithIndex at CloudStorageOperations.scala:1839), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.401720","level":"info","event":"25/07/20 04:12:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 34.6 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.406693","level":"info","event":"25/07/20 04:12:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 434.1 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.407635","level":"info","event":"25/07/20 04:12:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8da053d44e9f:46785 (size: 18.7 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.408333","level":"info","event":"25/07/20 04:12:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.423734","level":"info","event":"25/07/20 04:12:31 INFO DAGScheduler: Submitting 11 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at mapPartitionsWithIndex at CloudStorageOperations.scala:1839) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.424582","level":"info","event":"25/07/20 04:12:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 11 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.459043","level":"info","event":"25/07/20 04:12:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.24.0.5, executor 0, partition 0, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.462895","level":"info","event":"25/07/20 04:12:31 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.24.0.5, executor 0, partition 1, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.463102","level":"info","event":"25/07/20 04:12:31 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.24.0.5, executor 0, partition 2, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.463352","level":"info","event":"25/07/20 04:12:31 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.24.0.5, executor 0, partition 3, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.466017","level":"info","event":"25/07/20 04:12:31 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.24.0.5, executor 0, partition 4, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.466179","level":"info","event":"25/07/20 04:12:31 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.24.0.5, executor 0, partition 5, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.466240","level":"info","event":"25/07/20 04:12:31 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.24.0.5, executor 0, partition 6, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.466283","level":"info","event":"25/07/20 04:12:31 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.24.0.5, executor 0, partition 7, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.467524","level":"info","event":"25/07/20 04:12:31 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.24.0.5, executor 0, partition 8, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.468151","level":"info","event":"25/07/20 04:12:31 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.24.0.5, executor 0, partition 9, ANY, 7992 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.469131","level":"info","event":"25/07/20 04:12:31 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.24.0.5, executor 0, partition 10, ANY, 8152 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:31.700491","level":"info","event":"25/07/20 04:12:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.24.0.5:43519 (size: 18.7 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363307","level":"info","event":"25/07/20 04:12:33 ERROR Inbox: Ignoring error","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363438","level":"info","event":"java.io.NotSerializableException: org.apache.spark.storage.StorageStatus","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363482","level":"info","event":"Serialization stack:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363510","level":"info","event":"\t- object not serializable (class: org.apache.spark.storage.StorageStatus, value: org.apache.spark.storage.StorageStatus@2f656a0a)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363535","level":"info","event":"\t- element of array (index: 0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363560","level":"info","event":"\t- array (class [Lorg.apache.spark.storage.StorageStatus;, size 2)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363598","level":"info","event":"\tat org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363636","level":"info","event":"\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:49)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363669","level":"info","event":"\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363701","level":"info","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:286)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363736","level":"info","event":"\tat org.apache.spark.rpc.netty.RemoteNettyRpcCallContext.send(NettyRpcCallContext.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363775","level":"info","event":"\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363815","level":"info","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:162)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363853","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363890","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363925","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363961","level":"info","event":"\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.363998","level":"info","event":"\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.364035","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.364073","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.364106","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:833)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:33.453072","level":"info","event":"25/07/20 04:12:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.24.0.5:43519 (size: 34.1 KiB, free: 434.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:51.994449","level":"info","event":"25/07/20 04:12:51 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 20529 ms on 172.24.0.5 (executor 0) (1/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:53.055331","level":"info","event":"25/07/20 04:12:53 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 21586 ms on 172.24.0.5 (executor 0) (2/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:54.866267","level":"info","event":"25/07/20 04:12:54 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 23396 ms on 172.24.0.5 (executor 0) (3/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:54.989686","level":"info","event":"25/07/20 04:12:54 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 23527 ms on 172.24.0.5 (executor 0) (4/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:55.217488","level":"info","event":"25/07/20 04:12:55 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 23753 ms on 172.24.0.5 (executor 0) (5/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:55.578322","level":"info","event":"25/07/20 04:12:55 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 24115 ms on 172.24.0.5 (executor 0) (6/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:56.021827","level":"info","event":"25/07/20 04:12:56 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 24555 ms on 172.24.0.5 (executor 0) (7/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:56.032539","level":"info","event":"25/07/20 04:12:56 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 24567 ms on 172.24.0.5 (executor 0) (8/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:56.034164","level":"info","event":"25/07/20 04:12:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 24587 ms on 172.24.0.5 (executor 0) (9/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:56.148809","level":"info","event":"25/07/20 04:12:56 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 24683 ms on 172.24.0.5 (executor 0) (10/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.401168","level":"info","event":"25/07/20 04:12:57 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 25933 ms on 172.24.0.5 (executor 0) (11/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.402782","level":"info","event":"25/07/20 04:12:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.404292","level":"info","event":"25/07/20 04:12:57 INFO DAGScheduler: ResultStage 0 (collect at CloudStorageOperations.scala:1862) finished in 26.003 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.410897","level":"info","event":"25/07/20 04:12:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.411168","level":"info","event":"25/07/20 04:12:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.415734","level":"info","event":"25/07/20 04:12:57 INFO DAGScheduler: Job 0 finished: collect at CloudStorageOperations.scala:1862, took 23.239029 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.420510","level":"info","event":"25/07/20 04:12:57 INFO CloudStorageOperations$: Spark Connector Master: Finish uploading data for 11 partitions in 26.59 seconds.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.421596","level":"info","event":"25/07/20 04:12:57 INFO StageWriter$: writeToTableWithStagingTable: check table existence with \"RETAIL_SALES\".\"STAGING\".SILVER_DATA for SILVER_DATA","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.423395","level":"info","event":"25/07/20 04:12:57 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: desc table identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.593819","level":"info","event":"25/07/20 04:12:57 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: create   table if not exists identifier(?) (\"INVOICE_NO\" STRING ,\"STOCK_CODE\" STRING ,\"DESCRIPTION\" STRING ,\"QUANTITY\" INTEGER ,\"INVOICE_DATE\" STRING ,\"UNIT_PRICE\" DOUBLE ,\"CUSTOMER_ID\" STRING ,\"COUNTRY\" STRING ,\"REVENUE\" DOUBLE ,\"IS_RETURN\" INTEGER )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.901797","level":"info","event":"25/07/20 04:12:57 INFO StageWriter$: Begin to write at 2025-07-20T04:12:57.884568944 (Coordinated Universal Time)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.902449","level":"info","event":"25/07/20 04:12:57 INFO StageWriter$: Total file count is 11, non-empty files count is 11, total file size is 43.36 MB, total row count is 397.28 KB.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.906480","level":"info","event":"25/07/20 04:12:57 INFO StageWriter$: Now executing below command to write into table:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.906668","level":"info","event":"copy into SILVER_DATA_staging_1842685933 FROM @spark_connector_load_stage_gskbveJtDX/qWy9khrrUL/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.906750","level":"info","event":"FILE_FORMAT = (","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.906805","level":"info","event":"    TYPE=CSV","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.906858","level":"info","event":"    FIELD_DELIMITER='|'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.906904","level":"info","event":"    NULL_IF=()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.907014","level":"info","event":"    FIELD_OPTIONALLY_ENCLOSED_BY='\"'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.907137","level":"info","event":"    TIMESTAMP_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.907258","level":"info","event":"    DATE_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.907374","level":"info","event":"    BINARY_FORMAT=BASE64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.907483","level":"info","event":"  )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.907585","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.907686","level":"info","event":"25/07/20 04:12:57 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: copy into SILVER_DATA_staging_1842685933 FROM @spark_connector_load_stage_gskbveJtDX/qWy9khrrUL/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.907885","level":"info","event":"FILE_FORMAT = (","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.908064","level":"info","event":"    TYPE=CSV","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.908177","level":"info","event":"    FIELD_DELIMITER='|'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.908279","level":"info","event":"    NULL_IF=()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.908377","level":"info","event":"    FIELD_OPTIONALLY_ENCLOSED_BY='\"'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.908472","level":"info","event":"    TIMESTAMP_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.908569","level":"info","event":"    DATE_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.908669","level":"info","event":"    BINARY_FORMAT=BASE64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.908768","level":"info","event":"  )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:57.908870","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:58.069504","level":"info","event":"25/07/20 04:12:58 INFO SparkConnectorContext$: Spark connector register listener for: app-20250720041223-0001","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:58.071890","level":"info","event":"25/07/20 04:12:58 INFO SparkConnectorContext$: Add running query for app-20250720041223-0001 session: 182007829200902 queryId: 01bdce9c-0000-a19b-0000-a58900017be6","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:58.074900","level":"info","event":"25/07/20 04:12:58 INFO StageWriter$: The query ID for async writing into table command is: 01bdce9c-0000-a19b-0000-a58900017be6; The query ID URL is:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:12:58.075227","level":"info","event":"https://TRLIYPI-XK63730.snowflakecomputing.com/console#/monitoring/queries/detail?queryId=01bdce9c-0000-a19b-0000-a58900017be6","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:03.555910","level":"info","event":"25/07/20 04:13:03 INFO SparkConnectorContext$: Remove running query for app-20250720041223-0001 session: 182007829200902 queryId: 01bdce9c-0000-a19b-0000-a58900017be6","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:03.556547","level":"info","event":"25/07/20 04:13:03 INFO StageWriter$: First COPY command is done in 5.67 seconds at 2025-07-20T04:13:03.555421126, queryID is 01bdce9c-0000-a19b-0000-a58900017be6","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:03.560855","level":"info","event":"25/07/20 04:13:03 INFO StageWriter$: Succeed to write in 5.68 seconds at 2025-07-20T04:13:03.559841506","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:03.562373","level":"info","event":"25/07/20 04:13:03 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: alter table identifier(?) swap with identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:03.717299","level":"info","event":"25/07/20 04:13:03 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: drop table identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:03.906231","level":"info","event":"25/07/20 04:13:03 INFO StageWriter$: Spark Connector Master: Total job time is 33.07 seconds including read & upload time: 26.59 seconds and COPY time: 6.48 seconds.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:03.906569","level":"info","event":"25/07/20 04:13:03 WARN SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.4.0 with a connector designed to support Spark 3.3. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:03.911158","level":"info","event":"25/07/20 04:13:03 INFO ServerConnection$: Create ServerConnection with cached JDBC connection: 182007829200902","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:03.911539","level":"info","event":"25/07/20 04:13:03 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: select * from SILVER_DATA where 1 = 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:04.909161","level":"info","event":"25/07/20 04:13:04 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:04.909333","level":"info","event":"25/07/20 04:13:04 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:04.910985","level":"info","event":"25/07/20 04:13:04 WARN SparkConnectorContext$: Finish cancelling all queries for app-20250720041223-0001","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:04.911218","level":"info","event":"25/07/20 04:13:04 INFO ServerConnection$: Close all 1 cached connection.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:04.930922","level":"info","event":"25/07/20 04:13:04 INFO SparkUI: Stopped Spark web UI at http://8da053d44e9f:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:04.941237","level":"info","event":"25/07/20 04:13:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 8da053d44e9f:46785 in memory (size: 18.7 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:04.949377","level":"info","event":"25/07/20 04:13:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.24.0.5:43519 in memory (size: 18.7 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:04.957683","level":"info","event":"25/07/20 04:13:04 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:04.957974","level":"info","event":"25/07/20 04:13:04 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:05.155797","level":"info","event":"25/07/20 04:13:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:05.171192","level":"info","event":"25/07/20 04:13:05 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:05.171453","level":"info","event":"25/07/20 04:13:05 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:05.175399","level":"info","event":"25/07/20 04:13:05 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:05.177992","level":"info","event":"25/07/20 04:13:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:05.187100","level":"info","event":"25/07/20 04:13:05 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:05.187336","level":"info","event":"25/07/20 04:13:05 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:05.187900","level":"info","event":"25/07/20 04:13:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-fe948a15-1d2b-4ae2-82cd-951f49112f5c","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:05.192392","level":"info","event":"25/07/20 04:13:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-c5b16231-b35d-4f49-81c9-9b324ffe7a29","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:05.196453","level":"info","event":"25/07/20 04:13:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-fe948a15-1d2b-4ae2-82cd-951f49112f5c/pyspark-139b4732-1ed1-46c8-868b-4638e04b1359","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:05.243518","level":"info","event":"Command exited with return code 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:13:05.244398","level":"info","event":"Pushing xcom","ti":"RuntimeTaskInstance(id=UUID('01982608-16ba-779f-9cb8-ef403efc95ca'), task_id='silver_layer_snowflake', dag_id='retail_pipeline', run_id='manual__2025-07-20T04:12:01.057023+00:00', try_number=1, map_index=-1, hostname='4f337bb38dfd', context_carrier=None, task=<Task(BashOperator): silver_layer_snowflake>, bundle_instance=LocalDagBundle(name=dags-folder), max_tries=0, start_date=datetime.datetime(2025, 7, 20, 4, 12, 20, 280798, tzinfo=TzInfo(UTC)), end_date=None, is_mapped=False)","logger":"task"}
