{"timestamp":"2025-07-20T04:26:43.318901","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-07-20T04:26:43.319535","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/upload_to_hdfs_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-07-20T04:26:43.333893","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:43.334485","level":"info","event":"Running command: ['/usr/bin/bash', '-c', 'docker exec retail-spark-master                        /opt/bitnami/spark/bin/spark-submit                        --master spark://spark-master:7077 /opt/spark-apps/scripts/snowflake_staging.py']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:43.341670","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.281943","level":"info","event":"25/07/20 04:26:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.719286","level":"info","event":"Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.725097","level":"info","event":"25/07/20 04:26:44 INFO SparkContext: Running Spark version 3.1.1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.754354","level":"info","event":"25/07/20 04:26:44 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.754611","level":"info","event":"25/07/20 04:26:44 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.754689","level":"info","event":"25/07/20 04:26:44 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.754855","level":"info","event":"25/07/20 04:26:44 INFO SparkContext: Submitted application: Retail_Snowflake_Staging","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.771017","level":"info","event":"25/07/20 04:26:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.778912","level":"info","event":"25/07/20 04:26:44 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.779219","level":"info","event":"25/07/20 04:26:44 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.817390","level":"info","event":"25/07/20 04:26:44 INFO SecurityManager: Changing view acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.817520","level":"info","event":"25/07/20 04:26:44 INFO SecurityManager: Changing modify acls to: spark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.817563","level":"info","event":"25/07/20 04:26:44 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.817592","level":"info","event":"25/07/20 04:26:44 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.817622","level":"info","event":"25/07/20 04:26:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:44.984564","level":"info","event":"25/07/20 04:26:44 INFO Utils: Successfully started service 'sparkDriver' on port 38315.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.008709","level":"info","event":"25/07/20 04:26:45 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.033240","level":"info","event":"25/07/20 04:26:45 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.050577","level":"info","event":"25/07/20 04:26:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.050741","level":"info","event":"25/07/20 04:26:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.054549","level":"info","event":"25/07/20 04:26:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.067424","level":"info","event":"25/07/20 04:26:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-17d4c855-93dc-41a2-82c0-61a87e53ca78","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.083668","level":"info","event":"25/07/20 04:26:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.096317","level":"info","event":"25/07/20 04:26:45 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.260019","level":"info","event":"25/07/20 04:26:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.300578","level":"info","event":"25/07/20 04:26:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://40b3ac35ea26:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.444823","level":"info","event":"25/07/20 04:26:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://retail-spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.485863","level":"info","event":"25/07/20 04:26:45 INFO TransportClientFactory: Successfully created connection to retail-spark-master/172.24.0.3:7077 after 26 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.554262","level":"info","event":"25/07/20 04:26:45 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250720042645-0003","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.555657","level":"info","event":"25/07/20 04:26:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250720042645-0003/0 on worker-20250720041936-172.24.0.5-33199 (172.24.0.5:33199) with 12 core(s)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.556882","level":"info","event":"25/07/20 04:26:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250720042645-0003/0 on hostPort 172.24.0.5:33199 with 12 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.560994","level":"info","event":"25/07/20 04:26:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43555.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.561127","level":"info","event":"25/07/20 04:26:45 INFO NettyBlockTransferService: Server created on 40b3ac35ea26:43555","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.563185","level":"info","event":"25/07/20 04:26:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.571058","level":"info","event":"25/07/20 04:26:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 40b3ac35ea26, 43555, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.574912","level":"info","event":"25/07/20 04:26:45 INFO BlockManagerMasterEndpoint: Registering block manager 40b3ac35ea26:43555 with 366.3 MiB RAM, BlockManagerId(driver, 40b3ac35ea26, 43555, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.577620","level":"info","event":"25/07/20 04:26:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 40b3ac35ea26, 43555, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.578348","level":"info","event":"25/07/20 04:26:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 40b3ac35ea26, 43555, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.583597","level":"info","event":"25/07/20 04:26:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250720042645-0003/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.744337","level":"info","event":"25/07/20 04:26:45 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.944079","level":"info","event":"25/07/20 04:26:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/bitnami/spark/spark-warehouse').","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:45.944230","level":"info","event":"25/07/20 04:26:45 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:47.111369","level":"info","event":"25/07/20 04:26:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.24.0.5:59652) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:47.191012","level":"info","event":"25/07/20 04:26:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.24.0.5:37049 with 366.3 MiB RAM, BlockManagerId(0, 172.24.0.5, 37049, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:47.202898","level":"info","event":"25/07/20 04:26:47 INFO InMemoryFileIndex: It took 59 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:48.733970","level":"info","event":"25/07/20 04:26:48 WARN SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.1.1 with a connector designed to support Spark 3.3. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:48.787080","level":"info","event":"25/07/20 04:26:48 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:48.787343","level":"info","event":"25/07/20 04:26:48 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:48.789750","level":"info","event":"25/07/20 04:26:48 INFO FileSourceStrategy: Output Data Schema: struct<invoice_no: string, stock_code: string, description: string, quantity: int, invoice_date: string ... 8 more fields>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:48.901284","level":"info","event":"25/07/20 04:26:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:48.955434","level":"info","event":"25/07/20 04:26:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:48.979318","level":"info","event":"25/07/20 04:26:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 40b3ac35ea26:43555 (size: 27.4 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:48.983875","level":"info","event":"25/07/20 04:26:48 INFO SparkContext: Created broadcast 0 from rdd at SnowflakeWriter.scala:87","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:48.999125","level":"info","event":"25/07/20 04:26:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7576840 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:49.146320","level":"info","event":"25/07/20 04:26:49 WARN ServerConnection$: JDBC 3.13.31 is being used. But the certified JDBC version 3.13.30 is recommended.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.740804","level":"info","event":"25/07/20 04:26:50 INFO ServerConnection$: Create ServerConnection with new JDBC connection: 182007829188858","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760323","level":"info","event":"25/07/20 04:26:50 INFO SparkConnectorContext$: Spark Connector system config: {","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760494","level":"info","event":"  \"spark_connector_version\" : \"2.12.0\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760558","level":"info","event":"  \"spark_version\" : \"3.1.1\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760601","level":"info","event":"  \"application_name\" : \"Retail_Snowflake_Staging\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760652","level":"info","event":"  \"scala_version\" : \"2.12.10\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760689","level":"info","event":"  \"java_version\" : \"1.8.0_292\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760731","level":"info","event":"  \"jdbc_version\" : \"3.13.31\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760764","level":"info","event":"  \"certified_jdbc_version\" : \"3.13.30\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760799","level":"info","event":"  \"os_name\" : \"Linux\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760831","level":"info","event":"  \"max_memory_in_mb\" : 910,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760868","level":"info","event":"  \"total_memory_in_mb\" : 542,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760923","level":"info","event":"  \"free_memory_in_mb\" : 281,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.760981","level":"info","event":"  \"cpu_cores\" : 12,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761071","level":"info","event":"  \"spark_application_id\" : \"app-20250720042645-0003\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761143","level":"info","event":"  \"spark_language\" : \"Scala\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761211","level":"info","event":"  \"is_pyspark\" : false,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761261","level":"info","event":"  \"spark_config\" : {","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761310","level":"info","event":"    \"spark.rdd.compress\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761364","level":"info","event":"    \"spark.sql.warehouse.dir\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761410","level":"info","event":"    \"spark.app.name\" : \"Retail_Snowflake_Staging\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761453","level":"info","event":"    \"spark.serializer.objectStreamReset\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761499","level":"info","event":"    \"spark.driver.host\" : \"40b3ac35ea26\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761553","level":"info","event":"    \"spark.app.id\" : \"app-20250720042645-0003\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761610","level":"info","event":"    \"spark.submit.pyFiles\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761659","level":"info","event":"    \"spark.app.startTime\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761705","level":"info","event":"    \"spark.executor.id\" : \"driver\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761794","level":"info","event":"    \"spark.master\" : \"spark://retail-spark-master:7077\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761856","level":"info","event":"    \"spark.driver.port\" : \"N/A\",","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761917","level":"info","event":"    \"spark.submit.deployMode\" : \"client\"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.761984","level":"info","event":"  },","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.762075","level":"info","event":"  \"libraries\" : [ \"py4j\", \"py4j.commands\", \"py4j.reflection\", \"sun.reflect\", \"org.apache.spark.rdd\" ],","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.762153","level":"info","event":"  \"dependencies\" : [ ],","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.762202","level":"info","event":"  \"cluster_node_count\" : 2,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.762237","level":"info","event":"  \"spark_default_parallelism\" : 12,","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.762271","level":"info","event":"  \"deploy_mode\" : \"client\"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.762305","level":"info","event":"}","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:50.774091","level":"info","event":"25/07/20 04:26:50 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: alter session set timezone = 'Etc/UTC' , timestamp_ntz_output_format = 'YYYY-MM-DD HH24:MI:SS.FF3', timestamp_ltz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3', timestamp_tz_output_format = 'TZHTZM YYYY-MM-DD HH24:MI:SS.FF3' ;","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:51.367824","level":"info","event":"25/07/20 04:26:51 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: create  temporary stage if not exists identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.264779","level":"info","event":"25/07/20 04:26:52 INFO CloudStorageOperations$: Spark Connector Master: Begin to process and upload data for 11 partitions: directory=BHnWGVdM8x CSV true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.265136","level":"info","event":"25/07/20 04:26:52 INFO CloudStorageOperations$: Spark Connector Master: Begin to retrieve pre-signed URL or down-scoped token for 11 files by calling PUT command.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.544923","level":"info","event":"25/07/20 04:26:52 INFO CloudStorageOperations$: Spark Connector Master: Upload file to GCP with down-scoped token instead of pre-signed URL.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.547203","level":"info","event":"25/07/20 04:26:52 INFO CloudStorageOperations$: Spark Connector Master: Time to retrieve down-scoped token for 1/11 files is 281 ms.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.659712","level":"info","event":"25/07/20 04:26:52 INFO SparkContext: Starting job: collect at CloudStorageOperations.scala:1862","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.672863","level":"info","event":"25/07/20 04:26:52 INFO DAGScheduler: Got job 0 (collect at CloudStorageOperations.scala:1862) with 11 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.673029","level":"info","event":"25/07/20 04:26:52 INFO DAGScheduler: Final stage: ResultStage 0 (collect at CloudStorageOperations.scala:1862)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.673323","level":"info","event":"25/07/20 04:26:52 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.674852","level":"info","event":"25/07/20 04:26:52 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.679550","level":"info","event":"25/07/20 04:26:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at mapPartitionsWithIndex at CloudStorageOperations.scala:1839), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.698689","level":"info","event":"25/07/20 04:26:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 23.2 KiB, free 366.0 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.700956","level":"info","event":"25/07/20 04:26:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 13.8 KiB, free 365.9 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.701208","level":"info","event":"25/07/20 04:26:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 40b3ac35ea26:43555 (size: 13.8 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.701481","level":"info","event":"25/07/20 04:26:52 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.715732","level":"info","event":"25/07/20 04:26:52 INFO DAGScheduler: Submitting 11 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at mapPartitionsWithIndex at CloudStorageOperations.scala:1839) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.716670","level":"info","event":"25/07/20 04:26:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 11 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.746972","level":"info","event":"25/07/20 04:26:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.24.0.5, executor 0, partition 0, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.750216","level":"info","event":"25/07/20 04:26:52 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.24.0.5, executor 0, partition 1, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.751148","level":"info","event":"25/07/20 04:26:52 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.24.0.5, executor 0, partition 2, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.752001","level":"info","event":"25/07/20 04:26:52 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.24.0.5, executor 0, partition 3, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.752595","level":"info","event":"25/07/20 04:26:52 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.24.0.5, executor 0, partition 4, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.753008","level":"info","event":"25/07/20 04:26:52 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.24.0.5, executor 0, partition 5, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.753778","level":"info","event":"25/07/20 04:26:52 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.24.0.5, executor 0, partition 6, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.753965","level":"info","event":"25/07/20 04:26:52 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.24.0.5, executor 0, partition 7, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.754846","level":"info","event":"25/07/20 04:26:52 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.24.0.5, executor 0, partition 8, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.755288","level":"info","event":"25/07/20 04:26:52 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.24.0.5, executor 0, partition 9, ANY, 4937 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.755959","level":"info","event":"25/07/20 04:26:52 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.24.0.5, executor 0, partition 10, ANY, 5075 bytes) taskResourceAssignments Map()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:52.978029","level":"info","event":"25/07/20 04:26:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.24.0.5:37049 (size: 13.8 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.132797","level":"info","event":"25/07/20 04:26:55 ERROR Inbox: Ignoring error","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.132990","level":"info","event":"java.io.NotSerializableException: org.apache.spark.storage.StorageStatus","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133063","level":"info","event":"Serialization stack:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133116","level":"info","event":"\t- object not serializable (class: org.apache.spark.storage.StorageStatus, value: org.apache.spark.storage.StorageStatus@47a2bb9a)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133165","level":"info","event":"\t- element of array (index: 0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133209","level":"info","event":"\t- array (class [Lorg.apache.spark.storage.StorageStatus;, size 2)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133259","level":"info","event":"\tat org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133299","level":"info","event":"\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133341","level":"info","event":"\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133379","level":"info","event":"\tat org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:286)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133410","level":"info","event":"\tat org.apache.spark.rpc.netty.RemoteNettyRpcCallContext.send(NettyRpcCallContext.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133437","level":"info","event":"\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133464","level":"info","event":"\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:147)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133490","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133526","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133562","level":"info","event":"\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133592","level":"info","event":"\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133620","level":"info","event":"\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133647","level":"info","event":"\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133673","level":"info","event":"\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133744","level":"info","event":"\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133798","level":"info","event":"\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.133846","level":"info","event":"\tat java.lang.Thread.run(Thread.java:748)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:26:55.228497","level":"info","event":"25/07/20 04:26:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.24.0.5:37049 (size: 27.4 KiB, free: 366.3 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:14.544181","level":"info","event":"25/07/20 04:27:14 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 21789 ms on 172.24.0.5 (executor 0) (1/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:15.329509","level":"info","event":"25/07/20 04:27:15 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 22579 ms on 172.24.0.5 (executor 0) (2/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:15.948122","level":"info","event":"25/07/20 04:27:15 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 23192 ms on 172.24.0.5 (executor 0) (3/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:16.285817","level":"info","event":"25/07/20 04:27:16 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 23530 ms on 172.24.0.5 (executor 0) (4/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:16.733597","level":"info","event":"25/07/20 04:27:16 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 23976 ms on 172.24.0.5 (executor 0) (5/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:17.170759","level":"info","event":"25/07/20 04:27:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 24432 ms on 172.24.0.5 (executor 0) (6/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:17.295341","level":"info","event":"25/07/20 04:27:17 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 24543 ms on 172.24.0.5 (executor 0) (7/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:17.301109","level":"info","event":"25/07/20 04:27:17 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 24551 ms on 172.24.0.5 (executor 0) (8/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:17.303125","level":"info","event":"25/07/20 04:27:17 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 24550 ms on 172.24.0.5 (executor 0) (9/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:17.450674","level":"info","event":"25/07/20 04:27:17 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 24698 ms on 172.24.0.5 (executor 0) (10/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.173377","level":"info","event":"25/07/20 04:27:19 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 26418 ms on 172.24.0.5 (executor 0) (11/11)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.174453","level":"info","event":"25/07/20 04:27:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.174933","level":"info","event":"25/07/20 04:27:19 INFO DAGScheduler: ResultStage 0 (collect at CloudStorageOperations.scala:1862) finished in 26.478 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.178474","level":"info","event":"25/07/20 04:27:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.178578","level":"info","event":"25/07/20 04:27:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.181097","level":"info","event":"25/07/20 04:27:19 INFO DAGScheduler: Job 0 finished: collect at CloudStorageOperations.scala:1862, took 23.282442 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.183561","level":"info","event":"25/07/20 04:27:19 INFO CloudStorageOperations$: Spark Connector Master: Finish uploading data for 11 partitions in 26.92 seconds.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.184260","level":"info","event":"25/07/20 04:27:19 INFO StageWriter$: writeToTableWithStagingTable: check table existence with \"RETAIL_SALES\".\"STAGING\".SILVER_DATA for SILVER_DATA","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.184703","level":"info","event":"25/07/20 04:27:19 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: desc table identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.351260","level":"info","event":"25/07/20 04:27:19 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: create   table if not exists identifier(?) (\"INVOICE_NO\" STRING ,\"STOCK_CODE\" STRING ,\"DESCRIPTION\" STRING ,\"QUANTITY\" INTEGER ,\"INVOICE_DATE\" STRING ,\"UNIT_PRICE\" DOUBLE ,\"CUSTOMER_ID\" STRING ,\"COUNTRY\" STRING ,\"REVENUE\" DOUBLE ,\"IS_RETURN\" INTEGER )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.646214","level":"info","event":"25/07/20 04:27:19 INFO StageWriter$: Begin to write at 2025-07-20T04:27:19.638 (Coordinated Universal Time)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.647251","level":"info","event":"25/07/20 04:27:19 INFO StageWriter$: Total file count is 11, non-empty files count is 11, total file size is 43.36 MB, total row count is 397.28 KB.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.651797","level":"info","event":"25/07/20 04:27:19 INFO StageWriter$: Now executing below command to write into table:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.652054","level":"info","event":"copy into SILVER_DATA_staging_315023673 FROM @spark_connector_load_stage_AVgmVGP1jf/BHnWGVdM8x/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.652175","level":"info","event":"FILE_FORMAT = (","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.652263","level":"info","event":"    TYPE=CSV","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.652344","level":"info","event":"    FIELD_DELIMITER='|'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.652413","level":"info","event":"    NULL_IF=()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.652481","level":"info","event":"    FIELD_OPTIONALLY_ENCLOSED_BY='\"'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.652544","level":"info","event":"    TIMESTAMP_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.652618","level":"info","event":"    DATE_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.652725","level":"info","event":"    BINARY_FORMAT=BASE64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.652823","level":"info","event":"  )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.652908","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.652999","level":"info","event":"25/07/20 04:27:19 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: copy into SILVER_DATA_staging_315023673 FROM @spark_connector_load_stage_AVgmVGP1jf/BHnWGVdM8x/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.653165","level":"info","event":"FILE_FORMAT = (","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.653263","level":"info","event":"    TYPE=CSV","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.653333","level":"info","event":"    FIELD_DELIMITER='|'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.653399","level":"info","event":"    NULL_IF=()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.653498","level":"info","event":"    FIELD_OPTIONALLY_ENCLOSED_BY='\"'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.653747","level":"info","event":"    TIMESTAMP_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.653894","level":"info","event":"    DATE_FORMAT='TZHTZM YYYY-MM-DD HH24:MI:SS.FF9'","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.653988","level":"info","event":"    BINARY_FORMAT=BASE64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.654060","level":"info","event":"  )","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.654131","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.782389","level":"info","event":"25/07/20 04:27:19 INFO SparkConnectorContext$: Spark connector register listener for: app-20250720042645-0003","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.782751","level":"info","event":"25/07/20 04:27:19 INFO SparkConnectorContext$: Add running query for app-20250720042645-0003 session: 182007829188858 queryId: 01bdceab-0000-a1ec-0000-a5890001a022","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.784276","level":"info","event":"25/07/20 04:27:19 INFO StageWriter$: The query ID for async writing into table command is: 01bdceab-0000-a1ec-0000-a5890001a022; The query ID URL is:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:19.784465","level":"info","event":"https://TRLIYPI-XK63730.snowflakecomputing.com/console#/monitoring/queries/detail?queryId=01bdceab-0000-a1ec-0000-a5890001a022","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:23.120473","level":"info","event":"25/07/20 04:27:23 INFO SparkConnectorContext$: Remove running query for app-20250720042645-0003 session: 182007829188858 queryId: 01bdceab-0000-a1ec-0000-a5890001a022","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:23.121227","level":"info","event":"25/07/20 04:27:23 INFO StageWriter$: First COPY command is done in 3.48 seconds at 2025-07-20T04:27:23.119, queryID is 01bdceab-0000-a1ec-0000-a5890001a022","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:23.128111","level":"info","event":"25/07/20 04:27:23 INFO StageWriter$: Succeed to write in 3.49 seconds at 2025-07-20T04:27:23.126","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:23.128978","level":"info","event":"25/07/20 04:27:23 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: alter table identifier(?) swap with identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:23.269378","level":"info","event":"25/07/20 04:27:23 INFO SnowflakeSQLStatement: Spark Connector Master: execute query with bind variable: drop table identifier(?)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:23.431688","level":"info","event":"25/07/20 04:27:23 INFO StageWriter$: Spark Connector Master: Total job time is 31.17 seconds including read & upload time: 26.92 seconds and COPY time: 4.25 seconds.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:23.432037","level":"info","event":"25/07/20 04:27:23 WARN SnowflakeConnectorUtils$: Query pushdown is not supported because you are using Spark 3.1.1 with a connector designed to support Spark 3.3. Either use the version of Spark supported by the connector or install a version of the connector that supports your version of Spark.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.434732","level":"info","event":"25/07/20 04:27:24 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.437161","level":"info","event":"25/07/20 04:27:24 WARN SparkConnectorContext$: Finish cancelling all queries for app-20250720042645-0003","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.437701","level":"info","event":"25/07/20 04:27:24 INFO ServerConnection$: Close all 1 cached connection.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.447916","level":"info","event":"25/07/20 04:27:24 INFO SparkUI: Stopped Spark web UI at http://40b3ac35ea26:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.450443","level":"info","event":"25/07/20 04:27:24 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.450597","level":"info","event":"25/07/20 04:27:24 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.687422","level":"info","event":"25/07/20 04:27:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.703210","level":"info","event":"25/07/20 04:27:24 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.703332","level":"info","event":"25/07/20 04:27:24 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.710516","level":"info","event":"25/07/20 04:27:24 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.713244","level":"info","event":"25/07/20 04:27:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.721103","level":"info","event":"25/07/20 04:27:24 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.721295","level":"info","event":"25/07/20 04:27:24 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.721485","level":"info","event":"25/07/20 04:27:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-0fd85ae8-bfe9-4bda-86fe-a827952ba9a5","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.724281","level":"info","event":"25/07/20 04:27:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f1b91ed-0acf-4aff-b3fb-c4e89ccf6668","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.726689","level":"info","event":"25/07/20 04:27:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f1b91ed-0acf-4aff-b3fb-c4e89ccf6668/pyspark-7f81428c-ab9a-41a7-afb1-8d46b639f9be","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.765980","level":"info","event":"Command exited with return code 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-07-20T04:27:24.766549","level":"info","event":"Pushing xcom","ti":"RuntimeTaskInstance(id=UUID('01982615-44a4-743d-bd1e-7e9504ca4d11'), task_id='silver_layer_snowflake', dag_id='retail_pipeline', run_id='manual__2025-07-20T04:26:24.785497+00:00', try_number=1, map_index=-1, hostname='52b07f18d859', context_carrier=None, task=<Task(BashOperator): silver_layer_snowflake>, bundle_instance=LocalDagBundle(name=dags-folder), max_tries=0, start_date=datetime.datetime(2025, 7, 20, 4, 26, 43, 262101, tzinfo=TzInfo(UTC)), end_date=None, is_mapped=False)","logger":"task"}
